{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197a5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4843441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['PYTHONHASHSEED']=str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8769a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense,Input,Reshape, Flatten,ELU,RepeatVector,TimeDistributed, Bidirectional, PReLU, Concatenate, Subtract\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, multiply\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "import glob\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import r2_score\n",
    "from keras.utils import plot_model\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from keras.utils import to_categorical\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.stattools import pacf\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "# import seaborn as sns\n",
    "rcParams['figure.figsize']=15,5\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bb10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,7)})\n",
    "\n",
    "#For standardising the dat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba48218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b628af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations = ['Batseri kinnaur','Gharpa','ghoda_farm3_mandi','griffon peak 2','griffon_peak3',\n",
    "            'kuppa_data','nigulasridata','pagalnala_data','purbani_kinnaur','sanarli_1_mandi','sanarli_3_mandi',\n",
    "            'sandhol kangra','urni_dhank_kinnaur','griffon peak5 mandi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb253bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Column = ['Date','Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2786cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5a457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(model, Train,TrainL,Test, TestL):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(15, 5), sharey='row')\n",
    "    Predict=[]\n",
    "    True_cls=[]\n",
    "    test=Train\n",
    "    y=TrainL\n",
    "\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict=np.argmax(P,axis=1)\n",
    "    True_cls=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls, Predict, labels=[0,1])\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp1.plot(ax=axes[0])\n",
    "    \n",
    "    disp1.im_.colorbar.remove()\n",
    "\n",
    "    Predict2=[]\n",
    "    True_cls2=[]\n",
    "    test=Test\n",
    "    y=TestL\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict2=np.argmax(P,axis=1)\n",
    "    True_cls2=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls2, Predict2, labels=[0,1])\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp2.plot(ax=axes[1])\n",
    "    disp2.im_.colorbar.remove()\n",
    "    \n",
    "    # Evaluate model accuracy on training and testing sets\n",
    "    train_loss, train_acc = model.evaluate(Train, TrainL, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(Test, TestL, verbose=0)\n",
    "\n",
    "    # Display accuracy values on subplots\n",
    "    axes[0].set_title('Training Set\\nAccuracy: {:.2f}%'.format(train_acc * 100))\n",
    "    axes[1].set_title('Testing Set\\nAccuracy: {:.2f}%'.format(test_acc * 100))\n",
    "\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622c29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the Array\n",
    "def makeArray(Array):\n",
    "    New=np.array(Array[0])\n",
    "\n",
    "    for i in range(1,len(Array)):\n",
    "        New = np.append(New,Array[i],axis=0)\n",
    "        \n",
    "    return New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174c40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(Stations):\n",
    "    \n",
    "    Data, C = [], []\n",
    "    \n",
    "#     print(Stations)\n",
    "    file = Stations+'.csv'\n",
    "    newfile = 'deep_'+file\n",
    "    df = pd.read_csv('Deep_clean_dataset/'+newfile, header=0, index_col=None)\n",
    "    print(newfile)\n",
    "    df = df.reset_index(drop=True)\n",
    "    data=df[['Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']].values\n",
    "    data=data.astype('float32')\n",
    "    count=df['Count'].values\n",
    "    count=count.astype('float32')\n",
    "\n",
    "    #Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    Data.append(data)\n",
    "    C.append(count)\n",
    "   \n",
    "    return makeArray(Data), makeArray(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4f721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_Batseri kinnaur.csv\n",
      "deep_Gharpa.csv\n",
      "deep_ghoda_farm3_mandi.csv\n",
      "deep_griffon peak 2.csv\n",
      "deep_griffon_peak3.csv\n",
      "deep_kuppa_data.csv\n",
      "deep_nigulasridata.csv\n",
      "deep_pagalnala_data.csv\n",
      "deep_purbani_kinnaur.csv\n",
      "deep_sanarli_1_mandi.csv\n",
      "deep_sanarli_3_mandi.csv\n",
      "deep_sandhol kangra.csv\n",
      "deep_urni_dhank_kinnaur.csv\n",
      "deep_griffon peak5 mandi.csv\n"
     ]
    }
   ],
   "source": [
    "Data=[[] for x in range(len(Stations))]\n",
    "Count=[[] for x in range(len(Stations))]\n",
    "for i in range(len(Stations)):\n",
    "    Data[i], Count[i] = readData(Stations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef62ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packet(D, C, seq_len):\n",
    "    Packet_data = []\n",
    "    Packet_label = []\n",
    "    Packet_count = []\n",
    "\n",
    "    for i in range(len(D)-seq_len):\n",
    "        Packet_data.append(D[i:i+seq_len,:])\n",
    "        Packet_label.append(D[i+seq_len,:])\n",
    "        Packet_count.append(C[i+seq_len])\n",
    "\n",
    "    Packet_data = np.array(Packet_data)\n",
    "    Packet_label = np.array(Packet_label)\n",
    "    Packet_count = np.array(Packet_count).reshape(-1,1)\n",
    "    \n",
    "    return Packet_data, Packet_label, Packet_count\n",
    "\n",
    "def balance_data(d1, l1, c1):\n",
    "    P1 = []\n",
    "    L1 = []\n",
    "    C1 = []\n",
    "    count = 0\n",
    "    for i in range(len(c1)):\n",
    "        if c1[i]>0:\n",
    "            P1.append(d1[i])\n",
    "            L1.append(l1[i])\n",
    "            C1.append(c1[i])\n",
    "            count+=1\n",
    "\n",
    "            reset_random_seeds(seed)\n",
    "    idx = np.random.permutation(len(c1))        \n",
    "\n",
    "    P2 = []\n",
    "    L2 = []\n",
    "    C2 = []\n",
    "    idx1 = -1\n",
    "    while count!=0:\n",
    "        idx1+=1\n",
    "        if c1[idx[idx1]] == 0:\n",
    "            P2.append(d1[idx[idx1]])\n",
    "            L2.append(l1[idx[idx1]])\n",
    "            C2.append(c1[idx[idx1]])\n",
    "            count-=1\n",
    "\n",
    "    #Make the movement count to 1\n",
    "    #Comment this line if you want to movement count\n",
    "    C1=list(np.ones((len(C1))).reshape(-1,1))\n",
    "    \n",
    "    P1 = P1+P2\n",
    "    L1 = L1+L2\n",
    "    C1 = C1+C2\n",
    "    P1 = np.array(P1)\n",
    "    L1 = np.array(L1)\n",
    "    C1 = np.array(C1)\n",
    "    \n",
    "    return P1, L1, C1\n",
    "\n",
    "def mold(D):\n",
    "    T = []\n",
    "    for x in D:\n",
    "        for t in x:\n",
    "            T.append(t)\n",
    "            \n",
    "    return np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becb7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(Data, Count, seq_len):\n",
    "    print(\"Train:\")\n",
    "    print()\n",
    "    Train = []\n",
    "    Train_label = []\n",
    "    Train_count = []\n",
    "    S = [0,1,3,5,6,9,10]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Train.append(P1)\n",
    "        Train_label.append(L1)\n",
    "        Train_count.append(C1)\n",
    "\n",
    "    print(\"Test:\")\n",
    "    print()\n",
    "    Test = []\n",
    "    Test_label = []\n",
    "    Test_count = []\n",
    "    S = [2,7,8,4,11,12,13]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Test.append(P1)\n",
    "        Test_label.append(L1)\n",
    "        Test_count.append(C1)\n",
    "\n",
    "    Train = mold(Train)\n",
    "    Train_label = mold(Train_label)\n",
    "    Train_count = mold(Train_count)\n",
    "    Test = mold(Test)\n",
    "    Test_label = mold(Test_label)\n",
    "    Test_count = mold(Test_count)\n",
    "    \n",
    "    return Train, Train_label, Train_count, Test, Test_label, Test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1553dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "(6, 1)\n",
      "(2, 1)\n",
      "(950, 1)\n",
      "(20, 1)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "(2, 1)\n",
      "Test:\n",
      "\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(928, 1)\n",
      "(0,)\n",
      "(0,)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "Auto_Train, Auto_Train_label, Auto_Train_count, Auto_Test, Auto_Test_label, Auto_Test_count = train_test(Data, Count, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0217d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988, 10, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04dd1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "p=0.2\n",
    "b=5\n",
    "ft =13\n",
    "\n",
    "act='relu'\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "input_lyr = Input(shape=(ft,))\n",
    "# N = NoiseLayer(p)(input_lyr)\n",
    "E = Dense(10, activation=act)(input_lyr)\n",
    "# E=LeakyReLU(alpha=0.3)(E)\n",
    "\n",
    "Z = Dense(b, activation=act)(E)\n",
    "# E=LeakyReLU(alpha=0.3)(Z)\n",
    "\n",
    "E = Dense(10, activation=act)(Z)\n",
    "# E=LeakyReLU(alpha=0.3)(E)\n",
    "\n",
    "output_lyr = Dense(ft)(E)\n",
    "\n",
    "autoencoder = Model(input_lyr, output_lyr)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = Model(input_lyr, Z)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ac3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX = Auto_Train[:,-1]\n",
    "TestX = Auto_Test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3868ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainX = []\n",
    "# TestX = []\n",
    "\n",
    "# for x in Data:\n",
    "#     for y in x:\n",
    "#         TrainX.append(y)\n",
    "        \n",
    "# sz = int(len(TrainX)*0.8)\n",
    "# TestX = TrainX[sz:]\n",
    "# TrainX = TrainX[:sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "532cf143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 0.4220 - val_loss: 0.1306\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4134 - val_loss: 0.1291\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4052 - val_loss: 0.1276\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3975 - val_loss: 0.1262\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3903 - val_loss: 0.1249\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3835 - val_loss: 0.1237\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3770 - val_loss: 0.1226\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3709 - val_loss: 0.1215\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3651 - val_loss: 0.1205\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3597 - val_loss: 0.1196\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3545 - val_loss: 0.1188\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3496 - val_loss: 0.1180\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3449 - val_loss: 0.1173\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3405 - val_loss: 0.1166\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3363 - val_loss: 0.1160\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3322 - val_loss: 0.1154\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3284 - val_loss: 0.1148\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3247 - val_loss: 0.1142\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3212 - val_loss: 0.1137\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3179 - val_loss: 0.1132\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3147 - val_loss: 0.1126\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3116 - val_loss: 0.1121\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3086 - val_loss: 0.1116\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3057 - val_loss: 0.1111\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3030 - val_loss: 0.1107\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3003 - val_loss: 0.1102\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2977 - val_loss: 0.1097\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2952 - val_loss: 0.1092\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2927 - val_loss: 0.1087\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2904 - val_loss: 0.1082\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2880 - val_loss: 0.1077\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2858 - val_loss: 0.1072\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2835 - val_loss: 0.1067\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2813 - val_loss: 0.1063\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2791 - val_loss: 0.1058\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2770 - val_loss: 0.1053\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2748 - val_loss: 0.1048\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2725 - val_loss: 0.1044\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2703 - val_loss: 0.1039\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2680 - val_loss: 0.1034\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2657 - val_loss: 0.1030\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2633 - val_loss: 0.1025\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2609 - val_loss: 0.1021\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2584 - val_loss: 0.1017\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2558 - val_loss: 0.1013\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2531 - val_loss: 0.1009\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2504 - val_loss: 0.1005\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2476 - val_loss: 0.1001\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2448 - val_loss: 0.0998\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2419 - val_loss: 0.0994\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2390 - val_loss: 0.0991\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2361 - val_loss: 0.0987\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2332 - val_loss: 0.0984\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2304 - val_loss: 0.0980\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2275 - val_loss: 0.0976\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2247 - val_loss: 0.0972\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2219 - val_loss: 0.0968\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2192 - val_loss: 0.0965\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2165 - val_loss: 0.0961\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2138 - val_loss: 0.0957\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2112 - val_loss: 0.0953\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2086 - val_loss: 0.0949\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2060 - val_loss: 0.0945\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2036 - val_loss: 0.0941\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2011 - val_loss: 0.0937\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1987 - val_loss: 0.0932\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1963 - val_loss: 0.0928\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1939 - val_loss: 0.0923\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1916 - val_loss: 0.0919\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1893 - val_loss: 0.0913\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1870 - val_loss: 0.0908\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1847 - val_loss: 0.0903\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1824 - val_loss: 0.0897\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1801 - val_loss: 0.0891\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1778 - val_loss: 0.0885\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1755 - val_loss: 0.0878\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1732 - val_loss: 0.0872\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1709 - val_loss: 0.0865\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1686 - val_loss: 0.0858\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1662 - val_loss: 0.0850\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1639 - val_loss: 0.0843\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1615 - val_loss: 0.0835\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1591 - val_loss: 0.0828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1567 - val_loss: 0.0820\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1543 - val_loss: 0.0812\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1519 - val_loss: 0.0804\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1495 - val_loss: 0.0796\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1471 - val_loss: 0.0788\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1447 - val_loss: 0.0779\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1423 - val_loss: 0.0771\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1399 - val_loss: 0.0763\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1376 - val_loss: 0.0755\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1352 - val_loss: 0.0747\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1328 - val_loss: 0.0739\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1305 - val_loss: 0.0732\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1281 - val_loss: 0.0724\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1257 - val_loss: 0.0717\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1234 - val_loss: 0.0710\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1211 - val_loss: 0.0703\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1188 - val_loss: 0.0696\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1166 - val_loss: 0.0690\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1143 - val_loss: 0.0684\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1121 - val_loss: 0.0679\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1099 - val_loss: 0.0674\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1077 - val_loss: 0.0669\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1055 - val_loss: 0.0665\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1033 - val_loss: 0.0662\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1012 - val_loss: 0.0659\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0991 - val_loss: 0.0656\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0971 - val_loss: 0.0654\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0951 - val_loss: 0.0652\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0932 - val_loss: 0.0650\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0913 - val_loss: 0.0649\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0895 - val_loss: 0.0648\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0877 - val_loss: 0.0647\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0861 - val_loss: 0.0646\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0844 - val_loss: 0.0645\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0828 - val_loss: 0.0645\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0813 - val_loss: 0.0644\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0798 - val_loss: 0.0644\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0784 - val_loss: 0.0643\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0770 - val_loss: 0.0643\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0757 - val_loss: 0.0643\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0744 - val_loss: 0.0642\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0732 - val_loss: 0.0641\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0720 - val_loss: 0.0641\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0708 - val_loss: 0.0640\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0697 - val_loss: 0.0639\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0686 - val_loss: 0.0638\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0675 - val_loss: 0.0637\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0665 - val_loss: 0.0636\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0655 - val_loss: 0.0635\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0645 - val_loss: 0.0633\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0636 - val_loss: 0.0632\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0627 - val_loss: 0.0631\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0618 - val_loss: 0.0629\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0609 - val_loss: 0.0628\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0601 - val_loss: 0.0627\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0594 - val_loss: 0.0626\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0586 - val_loss: 0.0625\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0579 - val_loss: 0.0624\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0572 - val_loss: 0.0623\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0565 - val_loss: 0.0622\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0559 - val_loss: 0.0621\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0553 - val_loss: 0.0620\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0547 - val_loss: 0.0620\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0542 - val_loss: 0.0619\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0537 - val_loss: 0.0619\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0532 - val_loss: 0.0618\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0527 - val_loss: 0.0618\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0523 - val_loss: 0.0618\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0519 - val_loss: 0.0617\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0515 - val_loss: 0.0617\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0511 - val_loss: 0.0617\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0507 - val_loss: 0.0617\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0504 - val_loss: 0.0617\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0501 - val_loss: 0.0618\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0498 - val_loss: 0.0618\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0495 - val_loss: 0.0618\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0492 - val_loss: 0.0618\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0489 - val_loss: 0.0619\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0486 - val_loss: 0.0619\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0484 - val_loss: 0.0620\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0482 - val_loss: 0.0620\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0479 - val_loss: 0.0621\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0477 - val_loss: 0.0621\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0475 - val_loss: 0.0622\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0473 - val_loss: 0.0622\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0471 - val_loss: 0.0623\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0469 - val_loss: 0.0624\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0467 - val_loss: 0.0624\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0465 - val_loss: 0.0625\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0463 - val_loss: 0.0626\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0462 - val_loss: 0.0626\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0460 - val_loss: 0.0627\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0458 - val_loss: 0.0628\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0457 - val_loss: 0.0629\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0455 - val_loss: 0.0629\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0454 - val_loss: 0.0630\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0453 - val_loss: 0.0631\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0451 - val_loss: 0.0632\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0450 - val_loss: 0.0633\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0449 - val_loss: 0.0634\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0448 - val_loss: 0.0635\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0447 - val_loss: 0.0636\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0446 - val_loss: 0.0636\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0445 - val_loss: 0.0637\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0444 - val_loss: 0.0638\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0443 - val_loss: 0.0639\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0442 - val_loss: 0.0640\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0442 - val_loss: 0.0641\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0441 - val_loss: 0.0642\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0440 - val_loss: 0.0644\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0439 - val_loss: 0.0645\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0438 - val_loss: 0.0646\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0438 - val_loss: 0.0648\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0437 - val_loss: 0.0649\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0436 - val_loss: 0.0650\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0435 - val_loss: 0.0652\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0435 - val_loss: 0.0653\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0434 - val_loss: 0.0654\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0433 - val_loss: 0.0654\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0433 - val_loss: 0.0655\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0432 - val_loss: 0.0656\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0432 - val_loss: 0.0656\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0431 - val_loss: 0.0656\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0430 - val_loss: 0.0657\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0430 - val_loss: 0.0657\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0429 - val_loss: 0.0657\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0428 - val_loss: 0.0657\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0428 - val_loss: 0.0657\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0427 - val_loss: 0.0656\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0427 - val_loss: 0.0656\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0426 - val_loss: 0.0656\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0425 - val_loss: 0.0656\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0425 - val_loss: 0.0656\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0424 - val_loss: 0.0656\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0424 - val_loss: 0.0656\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0423 - val_loss: 0.0656\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0422 - val_loss: 0.0656\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0422 - val_loss: 0.0656\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0421 - val_loss: 0.0656\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0421 - val_loss: 0.0657\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0420 - val_loss: 0.0657\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0419 - val_loss: 0.0657\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0419 - val_loss: 0.0657\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0418 - val_loss: 0.0657\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0418 - val_loss: 0.0657\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0417 - val_loss: 0.0657\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0416 - val_loss: 0.0657\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0416 - val_loss: 0.0657\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0415 - val_loss: 0.0657\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0415 - val_loss: 0.0657\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0414 - val_loss: 0.0656\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0413 - val_loss: 0.0656\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0413 - val_loss: 0.0656\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0412 - val_loss: 0.0655\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0411 - val_loss: 0.0655\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0411 - val_loss: 0.0655\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0410 - val_loss: 0.0655\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0410 - val_loss: 0.0654\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0409 - val_loss: 0.0654\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0408 - val_loss: 0.0654\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0408 - val_loss: 0.0654\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0407 - val_loss: 0.0653\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0407 - val_loss: 0.0653\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0406 - val_loss: 0.0653\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0405 - val_loss: 0.0653\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0405 - val_loss: 0.0653\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0404 - val_loss: 0.0652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callback=keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1.0e-4, \n",
    "                                          patience=100, verbose=0, mode=\"auto\", \n",
    "                                       baseline=None, restore_best_weights=True)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "history = autoencoder.fit(TrainX, TrainX, epochs=500, batch_size=2048, \n",
    "                          validation_data=(TestX, TestX), verbose=1, callbacks=[callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa541013",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_Data = []\n",
    "for i in range(14):\n",
    "    D = Data[i]\n",
    "    Encoder_Data.append(encoder(D).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dcfc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "(6, 1)\n",
      "(2, 1)\n",
      "(950, 1)\n",
      "(20, 1)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "(2, 1)\n",
      "Test:\n",
      "\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(928, 1)\n",
      "(0,)\n",
      "(0,)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "LSTM_Train, LSTM_Train_label, LSTM_Train_count, LSTM_Test, LSTM_Test_label, LSTM_Test_count = train_test(Encoder_Data, Count, seq_len)\n",
    "LSTM_Train_count1=[]\n",
    "\n",
    "for x in LSTM_Train_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Train_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Train_count1.append([0,1])\n",
    "LSTM_Train_count = np.array(LSTM_Train_count1) \n",
    "\n",
    "LSTM_Test_count1=[]\n",
    "\n",
    "for x in LSTM_Test_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Test_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Test_count1.append([0,1])\n",
    "LSTM_Test_count = np.array(LSTM_Test_count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(seq_len, b)))\n",
    "# model.add(LSTM(500,activation='tanh',return_sequences=True))\n",
    "model.add(LSTM(100,activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080f453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6909 - accuracy: 0.5091 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6904 - accuracy: 0.5000 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6897 - accuracy: 0.5000 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6897 - accuracy: 0.5000 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6890 - accuracy: 0.5000 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6871 - accuracy: 0.5000 - val_loss: 0.6954 - val_accuracy: 0.4936\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6866 - accuracy: 0.5000 - val_loss: 0.6953 - val_accuracy: 0.4261\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6847 - accuracy: 0.5010 - val_loss: 0.6952 - val_accuracy: 0.4304\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6839 - accuracy: 0.5142 - val_loss: 0.6952 - val_accuracy: 0.4133\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6829 - accuracy: 0.5081 - val_loss: 0.6953 - val_accuracy: 0.4133\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6802 - accuracy: 0.5192 - val_loss: 0.6954 - val_accuracy: 0.4465\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6756 - accuracy: 0.5202 - val_loss: 0.6955 - val_accuracy: 0.5353\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6730 - accuracy: 0.5739 - val_loss: 0.6977 - val_accuracy: 0.5139\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6671 - accuracy: 0.6366 - val_loss: 0.7010 - val_accuracy: 0.5075\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6559 - accuracy: 0.6751 - val_loss: 0.7066 - val_accuracy: 0.5043\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6471 - accuracy: 0.6731 - val_loss: 0.7146 - val_accuracy: 0.5043\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6329 - accuracy: 0.6994 - val_loss: 0.7395 - val_accuracy: 0.5000\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6206 - accuracy: 0.6741 - val_loss: 0.7750 - val_accuracy: 0.5000\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6074 - accuracy: 0.6842 - val_loss: 0.8232 - val_accuracy: 0.5000\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5873 - accuracy: 0.6721 - val_loss: 0.9109 - val_accuracy: 0.5000\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5742 - accuracy: 0.6832 - val_loss: 0.9488 - val_accuracy: 0.5000\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5662 - accuracy: 0.7095 - val_loss: 1.0928 - val_accuracy: 0.5000\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5737 - accuracy: 0.7034 - val_loss: 1.0397 - val_accuracy: 0.5011\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5602 - accuracy: 0.7065 - val_loss: 1.0916 - val_accuracy: 0.5000\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5476 - accuracy: 0.7470 - val_loss: 1.0768 - val_accuracy: 0.5000\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5410 - accuracy: 0.7318 - val_loss: 1.0323 - val_accuracy: 0.5064\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5344 - accuracy: 0.7460 - val_loss: 1.0644 - val_accuracy: 0.5000\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5280 - accuracy: 0.7915 - val_loss: 1.0186 - val_accuracy: 0.5054\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5127 - accuracy: 0.7561 - val_loss: 1.0024 - val_accuracy: 0.5096\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5070 - accuracy: 0.7692 - val_loss: 1.0181 - val_accuracy: 0.5043\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5084 - accuracy: 0.8087 - val_loss: 0.9992 - val_accuracy: 0.5086\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4953 - accuracy: 0.7925 - val_loss: 0.9712 - val_accuracy: 0.5139\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4929 - accuracy: 0.7935 - val_loss: 0.9804 - val_accuracy: 0.5086\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4843 - accuracy: 0.8138 - val_loss: 0.9495 - val_accuracy: 0.5128\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4801 - accuracy: 0.8087 - val_loss: 0.9372 - val_accuracy: 0.5107\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4736 - accuracy: 0.8168 - val_loss: 0.9095 - val_accuracy: 0.5118\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4667 - accuracy: 0.8158 - val_loss: 0.8791 - val_accuracy: 0.5139\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4627 - accuracy: 0.8168 - val_loss: 0.8619 - val_accuracy: 0.5118\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4575 - accuracy: 0.8209 - val_loss: 0.8274 - val_accuracy: 0.5161\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4493 - accuracy: 0.8178 - val_loss: 0.8102 - val_accuracy: 0.5139\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4442 - accuracy: 0.8259 - val_loss: 0.7864 - val_accuracy: 0.5171\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4399 - accuracy: 0.8229 - val_loss: 0.7762 - val_accuracy: 0.5161\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4289 - accuracy: 0.8239 - val_loss: 0.7582 - val_accuracy: 0.5246\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4242 - accuracy: 0.8259 - val_loss: 0.7584 - val_accuracy: 0.5161\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4252 - accuracy: 0.8198 - val_loss: 0.7352 - val_accuracy: 0.5289\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4169 - accuracy: 0.8188 - val_loss: 0.7545 - val_accuracy: 0.5128\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4299 - accuracy: 0.8087 - val_loss: 0.7427 - val_accuracy: 0.5225\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4093 - accuracy: 0.8178 - val_loss: 0.7238 - val_accuracy: 0.5310\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3905 - accuracy: 0.8259 - val_loss: 0.7265 - val_accuracy: 0.5289\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3857 - accuracy: 0.8269 - val_loss: 0.6966 - val_accuracy: 0.5460\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3769 - accuracy: 0.8289 - val_loss: 0.7150 - val_accuracy: 0.5332\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3733 - accuracy: 0.8289 - val_loss: 0.6957 - val_accuracy: 0.5471\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3599 - accuracy: 0.8289 - val_loss: 0.6818 - val_accuracy: 0.5589\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3480 - accuracy: 0.8310 - val_loss: 0.6627 - val_accuracy: 0.5675\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3424 - accuracy: 0.8300 - val_loss: 0.6918 - val_accuracy: 0.5546\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3566 - accuracy: 0.8188 - val_loss: 0.7482 - val_accuracy: 0.5310\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4036 - accuracy: 0.8158 - val_loss: 0.7145 - val_accuracy: 0.5482\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4349 - accuracy: 0.7692 - val_loss: 0.7734 - val_accuracy: 0.5310\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5413 - accuracy: 0.7672 - val_loss: 0.8160 - val_accuracy: 0.5214\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4286 - accuracy: 0.7976 - val_loss: 0.6298 - val_accuracy: 0.5889\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5951 - accuracy: 0.7034 - val_loss: 0.7533 - val_accuracy: 0.5310\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4446 - accuracy: 0.8006 - val_loss: 0.8088 - val_accuracy: 0.5139\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4783 - accuracy: 0.7834 - val_loss: 0.7012 - val_accuracy: 0.5514\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3569 - accuracy: 0.8138 - val_loss: 0.6404 - val_accuracy: 0.5857\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4004 - accuracy: 0.7935 - val_loss: 0.7054 - val_accuracy: 0.5482\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3522 - accuracy: 0.8198 - val_loss: 0.7622 - val_accuracy: 0.5289\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3867 - accuracy: 0.8087 - val_loss: 0.7226 - val_accuracy: 0.5375\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3396 - accuracy: 0.8198 - val_loss: 0.6576 - val_accuracy: 0.5771\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3558 - accuracy: 0.8198 - val_loss: 0.6567 - val_accuracy: 0.5771\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3324 - accuracy: 0.8249 - val_loss: 0.7002 - val_accuracy: 0.5493\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3356 - accuracy: 0.8239 - val_loss: 0.7223 - val_accuracy: 0.5664\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3406 - accuracy: 0.8239 - val_loss: 0.6968 - val_accuracy: 0.5225\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3203 - accuracy: 0.8279 - val_loss: 0.6635 - val_accuracy: 0.5407\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3233 - accuracy: 0.8269 - val_loss: 0.6656 - val_accuracy: 0.4979\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3186 - accuracy: 0.8249 - val_loss: 0.6916 - val_accuracy: 0.4690\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3210 - accuracy: 0.8249 - val_loss: 0.6966 - val_accuracy: 0.4657\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3135 - accuracy: 0.8198 - val_loss: 0.6778 - val_accuracy: 0.4786\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3112 - accuracy: 0.8209 - val_loss: 0.6730 - val_accuracy: 0.4797\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3112 - accuracy: 0.8269 - val_loss: 0.6959 - val_accuracy: 0.4722\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3110 - accuracy: 0.8259 - val_loss: 0.6971 - val_accuracy: 0.4732\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3024 - accuracy: 0.8320 - val_loss: 0.6870 - val_accuracy: 0.4786\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3032 - accuracy: 0.8239 - val_loss: 0.6907 - val_accuracy: 0.4786\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.2965 - accuracy: 0.8320 - val_loss: 0.7091 - val_accuracy: 0.4690\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2943 - accuracy: 0.8563 - val_loss: 0.7095 - val_accuracy: 0.4722\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2978 - accuracy: 0.8674 - val_loss: 0.7060 - val_accuracy: 0.4754\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2965 - accuracy: 0.8634 - val_loss: 0.7080 - val_accuracy: 0.4754\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2964 - accuracy: 0.8735 - val_loss: 0.7287 - val_accuracy: 0.4625\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2945 - accuracy: 0.8907 - val_loss: 0.7189 - val_accuracy: 0.4679\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2942 - accuracy: 0.8917 - val_loss: 0.7124 - val_accuracy: 0.4764\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2899 - accuracy: 0.8907 - val_loss: 0.7313 - val_accuracy: 0.4657\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2946 - accuracy: 0.8988 - val_loss: 0.7320 - val_accuracy: 0.4657\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2857 - accuracy: 0.9038 - val_loss: 0.7121 - val_accuracy: 0.4775\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2937 - accuracy: 0.8968 - val_loss: 0.7285 - val_accuracy: 0.4679\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2912 - accuracy: 0.9069 - val_loss: 0.7418 - val_accuracy: 0.4625\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2941 - accuracy: 0.9089 - val_loss: 0.7214 - val_accuracy: 0.4764\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2868 - accuracy: 0.9109 - val_loss: 0.7215 - val_accuracy: 0.4764\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2877 - accuracy: 0.9099 - val_loss: 0.7409 - val_accuracy: 0.4647\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2894 - accuracy: 0.9140 - val_loss: 0.7356 - val_accuracy: 0.4679\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2864 - accuracy: 0.9119 - val_loss: 0.7256 - val_accuracy: 0.4764\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2924 - accuracy: 0.9140 - val_loss: 0.7375 - val_accuracy: 0.4711\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2849 - accuracy: 0.9160 - val_loss: 0.7349 - val_accuracy: 0.4711\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2849 - accuracy: 0.9160 - val_loss: 0.7346 - val_accuracy: 0.4732\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2811 - accuracy: 0.9140 - val_loss: 0.7333 - val_accuracy: 0.4754\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2852 - accuracy: 0.9160 - val_loss: 0.7495 - val_accuracy: 0.4657\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2775 - accuracy: 0.9170 - val_loss: 0.7549 - val_accuracy: 0.4647\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2826 - accuracy: 0.9211 - val_loss: 0.7533 - val_accuracy: 0.4657\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2743 - accuracy: 0.9241 - val_loss: 0.7570 - val_accuracy: 0.4647\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2854 - accuracy: 0.9170 - val_loss: 0.7684 - val_accuracy: 0.4615\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2735 - accuracy: 0.9261 - val_loss: 0.7592 - val_accuracy: 0.4657\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2773 - accuracy: 0.9211 - val_loss: 0.7622 - val_accuracy: 0.4657\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "H = model.fit(LSTM_Train, LSTM_Train_count, epochs=500,batch_size=512, validation_data=(LSTM_Test, LSTM_Test_count),callbacks=[EarlyStopping(monitor='val_accuracy', patience=50,restore_best_weights=True)],verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48e626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049dedb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix(model,LSTM_Train, LSTM_Train_count,LSTM_Test, LSTM_Test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ed303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb21632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowMac",
   "language": "python",
   "name": "tensorflowmac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
