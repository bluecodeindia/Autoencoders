{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8754ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e1cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['PYTHONHASHSEED']=str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00ddf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 22:32:35.140448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[43mseed\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model, save_model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense,Input,Reshape, Lambda,Add, Flatten,ELU,RepeatVector,TimeDistributed, Bidirectional, PReLU, Concatenate, Subtract\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense,Input,Reshape, Lambda,Add, Flatten,ELU,RepeatVector,TimeDistributed, Bidirectional, PReLU, Concatenate, Subtract\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, multiply\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,UpSampling2D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Nadam\n",
    "from keras.models import clone_model\n",
    "from keras import backend as K\n",
    "import glob\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import r2_score\n",
    "from keras.utils import plot_model\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from keras.utils import to_categorical\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.stattools import pacf\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "# import seaborn as sns\n",
    "rcParams['figure.figsize']=15,5\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3dd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,7)})\n",
    "\n",
    "#For standardising the dat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470a9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(arr):\n",
    "    \"\"\"\n",
    "    Convert a given array into one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        arr (numpy.ndarray): Input array to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded array.\n",
    "    \"\"\"\n",
    "    # Get the unique elements in the input array\n",
    "    unique_elements = np.unique(arr)\n",
    "    \n",
    "    # Create a dictionary to map unique elements to one-hot vectors\n",
    "    one_hot_dict = {elem: np.eye(len(unique_elements))[i] for i, elem in enumerate(unique_elements)}\n",
    "    \n",
    "    # Map the input array to one-hot vectors\n",
    "    one_hot_arr = np.array([one_hot_dict[elem] for elem in arr])\n",
    "    \n",
    "    return one_hot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e87990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3eec951",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations = os.listdir('Clean-dataset-LMS')\n",
    "# Stations.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c9c3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean_deep_LMS Data - Rulehr_Shahpur_Kangra.csv',\n",
       " 'clean_deep_LMS Data - griffon peak 4.csv',\n",
       " 'clean_deep_LMS Data - Ghoro_Farm_2.csv',\n",
       " 'clean_deep_LMS Data - Ghoda Farm 3.csv',\n",
       " 'clean_deep_LMS Data - dharamshala kangra.csv',\n",
       " 'clean_deep_LMS Data - Purbani_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - Ghoda Farm 5.csv',\n",
       " 'clean_deep_LMS Data - Garudnala_Mandi.csv',\n",
       " 'clean_deep_LMS Data - macleodganj kangra.csv',\n",
       " 'clean_deep_LMS Data - Sanarli_1_Mandi.csv',\n",
       " 'clean_deep_LMS Data - Pagal_Nala_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - Nigulsari_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - Sanarli_3_Mandi.csv',\n",
       " 'clean_deep_LMS Data - Urni_Dhank_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - Kuppa_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - Batseri_Kinnaur.csv',\n",
       " 'clean_deep_LMS Data - griffon peak 3.csv',\n",
       " 'clean_deep_LMS Data - griffon peak 5.csv',\n",
       " 'clean_deep_LMS Data - Sanarli_2_Mandi.csv',\n",
       " 'clean_deep_LMS Data - Ghoda Farm 4.csv',\n",
       " 'clean_deep_LMS Data - chola kangra.csv',\n",
       " 'clean_deep_LMS Data - Griffon peak 6.csv',\n",
       " 'clean_deep_LMS Data - Ghoro_Farm_1.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484fffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Column = ['Date','Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a148fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3030e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tSNE(Train, TrainL):\n",
    "    \"\"\"\n",
    "    Perform t-SNE dimensionality reduction on the given data and visualize it in an interactive 3D scatter plot using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        Train (numpy.ndarray): Input data array.\n",
    "        TrainL (numpy.ndarray): Labels for the input data array.\n",
    "    \"\"\"\n",
    "    x_subset = Train[:]\n",
    "    y_subset = [np.argmax(x) for x in TrainL[:]]\n",
    "\n",
    "    print(np.unique(y_subset))\n",
    "    if x_subset.ndim>=3:\n",
    "        x_subset = x_subset.reshape((x_subset.shape[0], x_subset.shape[1] * x_subset.shape[2]))\n",
    "    else:\n",
    "        x_subset=x_subset.reshape((x_subset.shape[0],x_subset.shape[1]))\n",
    "    \n",
    "    tsne = TSNE(random_state=42, n_components=3, verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset)\n",
    "    \n",
    "    # Create a 3D scatter plot using Scatter3d trace\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=tsne[:, 0],\n",
    "        y=tsne[:, 1],\n",
    "        z=tsne[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=y_subset,\n",
    "            colorscale='Spectral',\n",
    "            opacity=1,\n",
    "            line=dict(\n",
    "                width=0.5,\n",
    "                color='black'\n",
    "            )\n",
    "        ),\n",
    "        name='t-SNE'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "    title='Visualizing through t-SNE in 3D',\n",
    "    scene=dict(\n",
    "        xaxis_title='x',\n",
    "        yaxis_title='y',\n",
    "        zaxis_title='z'\n",
    "    ),\n",
    "    width=1000,  # Set width of the plot\n",
    "    height=800  # Set height of the plot\n",
    ")\n",
    "\n",
    "    # Create a separate plot with a colorbar to show the colors and their corresponding values\n",
    "    colorbar = go.Figure()\n",
    "    colorbar.add_trace(go.Scatter(\n",
    "        x=y_subset,\n",
    "        y=[0] * len(y_subset),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=y_subset,\n",
    "            colorscale='Spectral',\n",
    "            opacity=1,\n",
    "            colorbar=dict(\n",
    "                title='Class Label',\n",
    "                titleside='right',\n",
    "                tickvals=np.unique(y_subset),\n",
    "                ticktext=[str(i) for i in np.unique(y_subset)],\n",
    "                ticks='outside'\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    colorbar.update_layout(\n",
    "        title='Colorbar for t-SNE plot',\n",
    "        showlegend=False,\n",
    "        width=500,  # Set width of the plot\n",
    "        height=400  # Set height of the plot\n",
    "    )\n",
    "\n",
    "    # Display the plots side by side\n",
    "    \n",
    "    fig.show()\n",
    "    colorbar.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c404e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(model, Train,TrainL,Test, TestL):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(15, 5), sharey='row')\n",
    "    Predict=[]\n",
    "    True_cls=[]\n",
    "    test=Train\n",
    "    y=TrainL\n",
    "\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict=np.argmax(P,axis=1)\n",
    "    True_cls=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls, Predict, labels=[0,1])\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp1.plot(ax=axes[0])\n",
    "    \n",
    "    disp1.im_.colorbar.remove()\n",
    "    train_acc = (cm[0][0]+cm[1][1])/np.sum(cm)\n",
    "\n",
    "    Predict2=[]\n",
    "    True_cls2=[]\n",
    "    test=Test\n",
    "    y=TestL\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict2=np.argmax(P,axis=1)\n",
    "    True_cls2=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls2, Predict2, labels=[0,1])\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp2.plot(ax=axes[1])\n",
    "    disp2.im_.colorbar.remove()\n",
    "    \n",
    "    # Evaluate model accuracy on training and testing sets\n",
    "    \n",
    "    test_acc = (cm[0][0]+cm[1][1])/np.sum(cm)\n",
    "\n",
    "    # Display accuracy values on subplots\n",
    "    axes[0].set_title('Training Set\\nAccuracy: {:.2f}%'.format(train_acc * 100))\n",
    "    axes[1].set_title('Testing Set\\nAccuracy: {:.2f}%'.format(test_acc * 100))\n",
    "\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce91305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the Array\n",
    "def makeArray(Array):\n",
    "    New=np.array(Array[0])\n",
    "\n",
    "    for i in range(1,len(Array)):\n",
    "        New = np.append(New,Array[i],axis=0)\n",
    "        \n",
    "    return New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a56827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b896b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(Stations):\n",
    "    \n",
    "    Data, C = [], []\n",
    "    \n",
    "#     print(Stations)\n",
    "    file = Stations\n",
    "    newfile = file\n",
    "    df = pd.read_csv('Clean-dataset-LMS/'+newfile, header=0, index_col=None)\n",
    "    print(newfile)\n",
    "    df = df.reset_index(drop=True)\n",
    "    data=df[['Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']].values\n",
    "    data=data.astype('float32')\n",
    "    data[np.where(data[:,-1]>0),-1] = 1\n",
    "    count=df['Count'].values\n",
    "    count=count.astype('float32')\n",
    "\n",
    "    #Normalize the data\n",
    "    \n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    Data.append(data)\n",
    "    C.append(count)\n",
    "   \n",
    "    return makeArray(Data), makeArray(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "685db14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_deep_LMS Data - Rulehr_Shahpur_Kangra.csv\n",
      "clean_deep_LMS Data - griffon peak 4.csv\n",
      "clean_deep_LMS Data - Ghoro_Farm_2.csv\n",
      "clean_deep_LMS Data - Ghoda Farm 3.csv\n",
      "clean_deep_LMS Data - dharamshala kangra.csv\n",
      "clean_deep_LMS Data - Purbani_Kinnaur.csv\n",
      "clean_deep_LMS Data - Ghoda Farm 5.csv\n",
      "clean_deep_LMS Data - Garudnala_Mandi.csv\n",
      "clean_deep_LMS Data - macleodganj kangra.csv\n",
      "clean_deep_LMS Data - Sanarli_1_Mandi.csv\n",
      "clean_deep_LMS Data - Pagal_Nala_Kinnaur.csv\n",
      "clean_deep_LMS Data - Nigulsari_Kinnaur.csv\n",
      "clean_deep_LMS Data - Sanarli_3_Mandi.csv\n",
      "clean_deep_LMS Data - Urni_Dhank_Kinnaur.csv\n",
      "clean_deep_LMS Data - Kuppa_Kinnaur.csv\n",
      "clean_deep_LMS Data - Batseri_Kinnaur.csv\n",
      "clean_deep_LMS Data - griffon peak 3.csv\n",
      "clean_deep_LMS Data - griffon peak 5.csv\n",
      "clean_deep_LMS Data - Sanarli_2_Mandi.csv\n",
      "clean_deep_LMS Data - Ghoda Farm 4.csv\n",
      "clean_deep_LMS Data - chola kangra.csv\n",
      "clean_deep_LMS Data - Griffon peak 6.csv\n",
      "clean_deep_LMS Data - Ghoro_Farm_1.csv\n"
     ]
    }
   ],
   "source": [
    "Data=[[] for x in range(len(Stations))]\n",
    "Count=[[] for x in range(len(Stations))]\n",
    "for i in range(len(Stations)):\n",
    "    Data[i], Count[i] = readData(Stations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae73be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6144, 13) (6144, 13)\n",
      "(26345, 13) (26257, 13)\n",
      "(21080, 13) (21074, 13)\n",
      "(12820, 13) (12820, 13)\n",
      "(13050, 13) (13050, 13)\n",
      "(26834, 13) (26828, 13)\n",
      "(13697, 13) (13530, 13)\n",
      "(16053, 13) (16053, 13)\n",
      "(5384, 13) (5384, 13)\n",
      "(19, 13) (19, 13)\n",
      "(31122, 13) (31122, 13)\n",
      "(15402, 13) (13632, 13)\n",
      "(23740, 13) (23740, 13)\n",
      "(16053, 13) (16053, 13)\n",
      "(15402, 13) (13632, 13)\n",
      "(29409, 13) (29409, 13)\n",
      "(17856, 13) (15604, 13)\n",
      "(7479, 13) (7461, 13)\n",
      "(23740, 13) (23740, 13)\n",
      "(7589, 13) (6773, 13)\n",
      "(54, 13) (54, 13)\n",
      "(1295, 13) (1295, 13)\n",
      "(14825, 13) (14522, 13)\n"
     ]
    }
   ],
   "source": [
    "for i in range(23):\n",
    "    idx = i\n",
    "    print(Data[idx].shape,np.unique(Data[idx],axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb0974d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packet(D, C, seq_len):\n",
    "    Packet_data = []\n",
    "    Packet_label = []\n",
    "    Packet_count = []\n",
    "\n",
    "    for i in range(len(D)-seq_len):\n",
    "        Packet_data.append(D[i:i+seq_len,:])\n",
    "        Packet_label.append(D[i+seq_len,:])\n",
    "        Packet_count.append(C[i+seq_len-1])\n",
    "\n",
    "    Packet_data = np.array(Packet_data)\n",
    "    Packet_label = np.array(Packet_label)\n",
    "    Packet_count = np.array(Packet_count).reshape(-1,1)\n",
    "    \n",
    "    return Packet_data, Packet_label, Packet_count\n",
    "\n",
    "def balance_data(d1, l1, c1):\n",
    "    P1 = []\n",
    "    L1 = []\n",
    "    C1 = []\n",
    "    count = 0\n",
    "    for i in range(len(c1)):\n",
    "        if c1[i]>0:\n",
    "            P1.append(d1[i])\n",
    "            L1.append(l1[i])\n",
    "            C1.append(c1[i])\n",
    "            count+=1\n",
    "\n",
    "            reset_random_seeds(seed)\n",
    "    idx = np.random.permutation(len(c1))        \n",
    "\n",
    "    P2 = []\n",
    "    L2 = []\n",
    "    C2 = []\n",
    "    idx1 = -1\n",
    "    while count!=0:\n",
    "        idx1+=1\n",
    "        if c1[idx[idx1]] == 0:\n",
    "            P2.append(d1[idx[idx1]])\n",
    "            L2.append(l1[idx[idx1]])\n",
    "            C2.append(c1[idx[idx1]])\n",
    "            count-=1\n",
    "\n",
    "    #Make the movement count to 1\n",
    "    #Comment this line if you want to movement count\n",
    "    C1=list(np.ones((len(C1))).reshape(-1,1))\n",
    "    \n",
    "    P1 = P1+P2\n",
    "    L1 = L1+L2\n",
    "    C1 = C1+C2\n",
    "    P1 = np.array(P1)\n",
    "    L1 = np.array(L1)\n",
    "    C1 = np.array(C1)\n",
    "    \n",
    "    return P1, L1, C1\n",
    "\n",
    "def mold(D):\n",
    "    T = []\n",
    "    for x in D:\n",
    "        for t in x:\n",
    "            T.append(t)\n",
    "            \n",
    "    return np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0332d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(Data, Count, seq_len):\n",
    "    print(\"Train:\")\n",
    "    print()\n",
    "    Train = []\n",
    "    Train_label = []\n",
    "    Train_count = []\n",
    "    S = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Train.append(P1)\n",
    "        Train_label.append(L1)\n",
    "        Train_count.append(C1)\n",
    "\n",
    "    print(\"Test:\")\n",
    "    print()\n",
    "    Test = []\n",
    "    Test_label = []\n",
    "    Test_count = []\n",
    "    S = [13,14,16,17,18,19,21,22]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Test.append(P1)\n",
    "        Test_label.append(L1)\n",
    "        Test_count.append(C1)\n",
    "\n",
    "    Train = mold(Train)\n",
    "    Train_label = mold(Train_label)\n",
    "    Train_count = mold(Train_count)\n",
    "    Test = mold(Test)\n",
    "    Test_label = mold(Test_label)\n",
    "    Test_count = mold(Test_count)\n",
    "    \n",
    "    return Train, Train_label, Train_count, Test, Test_label, Test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b82b5e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "(1298, 1)\n",
      "(46, 1)\n",
      "(6, 1)\n",
      "(84, 1)\n",
      "(602, 1)\n",
      "(46, 1)\n",
      "(0,)\n",
      "(2, 1)\n",
      "(0,)\n",
      "(52, 1)\n",
      "(90, 1)\n",
      "(40, 1)\n",
      "Test:\n",
      "\n",
      "(0,)\n",
      "(90, 1)\n",
      "(2232, 1)\n",
      "(54, 1)\n",
      "(40, 1)\n",
      "(62, 1)\n",
      "(4, 1)\n",
      "(172, 1)\n"
     ]
    }
   ],
   "source": [
    "Auto_Train, Auto_Train_label, Auto_Train_count, Auto_Test, Auto_Test_label, Auto_Test_count = train_test(Data, Count, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d9b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "(1298, 1)\n",
      "(46, 1)\n",
      "(6, 1)\n",
      "(84, 1)\n",
      "(602, 1)\n",
      "(46, 1)\n",
      "(0,)\n",
      "(2, 1)\n",
      "(0,)\n",
      "(52, 1)\n",
      "(90, 1)\n",
      "(40, 1)\n",
      "Test:\n",
      "\n",
      "(0,)\n",
      "(90, 1)\n",
      "(2232, 1)\n",
      "(54, 1)\n",
      "(40, 1)\n",
      "(62, 1)\n",
      "(4, 1)\n",
      "(172, 1)\n"
     ]
    }
   ],
   "source": [
    "LSTM_Train, LSTM_Train_label, LSTM_Train_count, LSTM_Test, LSTM_Test_label, LSTM_Test_count = train_test(Data, Count, seq_len)\n",
    "LSTM_Train_count1=[]\n",
    "\n",
    "for x in LSTM_Train_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Train_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Train_count1.append([0,1])\n",
    "LSTM_Train_count = np.array(LSTM_Train_count1) \n",
    "\n",
    "LSTM_Test_count1=[]\n",
    "\n",
    "for x in LSTM_Test_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Test_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Test_count1.append([0,1])\n",
    "LSTM_Test_count = np.array(LSTM_Test_count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2fd1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Reshape, Conv1DTranspose, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e29646d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time-series data using the timeGAN model\n",
    "def generate_samples(timeGAN, latent_dim, num_samples):\n",
    "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
    "    generated = timeGAN.predict(noise)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ceff25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a10d9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (seq_len, 13)\n",
    "output_shape = input_shape\n",
    "latent_dim = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facff07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def get_angles(self, pos, i):\n",
    "        angle_rates = 1 / tf.pow(10000.0, tf.cast((2 * tf.cast(i // 2, tf.float32)) / tf.cast(self.embed_dim,tf.float32),tf.float32))\n",
    "        angle_rates = tf.cast(angle_rates, tf.float32)\n",
    "        pos = tf.cast(pos, tf.float32)\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def call(self, x):\n",
    "        # create position indices\n",
    "        pos = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "#         print(tf.expand_dims(pos, axis=-1))\n",
    "        # compute angles\n",
    "        angles = self.get_angles(tf.expand_dims(pos, axis=-1), tf.range(self.embed_dim, dtype=tf.float32))\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angles = tf.cast(angles, tf.float32)\n",
    "        angles = tf.where(tf.math.equal(tf.math.mod(tf.range(self.embed_dim), 2), 0), tf.sin(angles), tf.cos(angles))\n",
    "        # expand angles tensor to have the same shape as input tensor\n",
    "        angles = tf.repeat(tf.expand_dims(angles, axis=0), repeats=tf.shape(x)[0], axis=0)\n",
    "        # cast input tensor to float\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        angles = tf.cast(angles, tf.float32)\n",
    "        # add embeddings and input\n",
    "        return tf.add(x, angles)\n",
    "#         return tf.concat([x, angles], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c5a7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class AccelNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, output_dim, seq_len):\n",
    "        super(AccelNet, self).__init__()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Reshape((-1,)))\n",
    "        self.model.add(Dense(seq_len*hidden_dim))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(Dense(seq_len*output_dim))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(Reshape((seq_len,output_dim)))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba37111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class DispNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DispNet, self).__init__()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Reshape((-1,)))\n",
    "        self.model.add(Dense(seq_len*hidden_dim))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(Dense(seq_len*output_dim))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(Reshape((seq_len,output_dim)))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a62a028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "#         x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            )\n",
    "        x = self.add([x, attn_output])\n",
    "#         x = self.layernorm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff),\n",
    "            LeakyReLU(alpha=0.01),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "            LeakyReLU(alpha=0.01),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "#         x = self.layer_norm(x) \n",
    "        return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1,seq_len=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.pos_embedding = PositionalEmbedding(seq_len, 13)\n",
    "        self.encoding = AccelNet(2, d_model-6, seq_len)\n",
    "\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "#         x = self.encoding(x)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_embedding(x)\n",
    "      \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bb50da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size,  dropout_rate=0.1,seq_len=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate, seq_len=seq_len)\n",
    "        self.seq_len=seq_len\n",
    "        self.initializer = tf.keras.initializers.HeUniform()\n",
    "        self.flat=tf.keras.layers.Flatten()\n",
    "        self.dense1=tf.keras.layers.Dense(100, kernel_initializer=self.initializer, name='dense1')\n",
    "        self.dense2=tf.keras.layers.Dense(30, kernel_initializer=self.initializer, name='dense2')\n",
    "        self.out=tf.keras.layers.Dense(1, activation='relu', name='out')\n",
    "        self.LR=LeakyReLU(alpha=0.01)\n",
    "                \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context = inputs\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        x = self.flat(context)\n",
    "        x = self.dense1(x)\n",
    "        x = self.LR(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.LR(x)\n",
    "        logits = self.out(x)\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832edc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb2c791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_lstm(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(LSTM(hidden_units, activation='tanh', input_shape=(seq_len, 13), return_sequences=True, name='L{}'.format(i+2)))\n",
    "        elif i == num_layers-1:\n",
    "            model.add(LSTM(hidden_units, activation='tanh', input_shape=(seq_len, 13), return_sequences=False, name='L{}'.format(i+2)))\n",
    "        else:\n",
    "            model.add(LSTM(hidden_units, activation='tanh', return_sequences=True, name='L{}'.format(i+2)))\n",
    "    model.add(Dense(latent_dim, name='latent'))\n",
    "\n",
    "    z = Input(shape=(seq_len, 13), name='L1')\n",
    "    out = model(z)\n",
    "    return Model(z, out)\n",
    "\n",
    "\n",
    "def make_encoder_cnn(seq_len, latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, (3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Conv1D(32, (3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(latent_dim,activation=\"relu\"))\n",
    "    z = Input(shape=(seq_len,13), name='L1')\n",
    "    out = model(z)\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_decoder(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(RepeatVector(seq_len))\n",
    "    model.add(LSTM(50, activation='tanh',  return_sequences=True, name='L2'))\n",
    "    model.add(TimeDistributed(Dense(13),name='decoder'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(seq_len*13,activation=\"relu\"))\n",
    "    z = Input(shape=(latent_dim,), name='L1')\n",
    "    out = model(z)\n",
    "    \n",
    "    convt_model = Sequential()\n",
    "    convt_model.add(RepeatVector(seq_len))\n",
    "    convt_model.add(Conv1DTranspose(32, (3), strides=1, padding='same', activation='relu', input_shape=(1, latent_dim)))\n",
    "    convt_model.add(Conv1DTranspose(32, (3), strides=1, padding='same', activation='relu'))\n",
    "    convt_model.add(Conv1DTranspose(13, (3), padding='same', activation='linear'))\n",
    "    convt_model.add(Flatten())\n",
    "    convt_model.add(Dense(seq_len*13, activation=\"relu\"))\n",
    "    out2 = convt_model(z)\n",
    "    \n",
    "    concatenated = Concatenate()([out, out2])\n",
    "    out3 = Dense(seq_len*13)(concatenated)\n",
    "    out3 = Reshape((seq_len, 13,))(out3)\n",
    "    \n",
    "    return Model(z, out3)\n",
    "\n",
    "def make_generator(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(Bidirectional(LSTM(hidden_units, activation='tanh', input_shape=(seq_len,13), return_sequences=True, name='L{}'.format(i+2))))\n",
    "        elif i == num_layers-1:\n",
    "            model.add(Bidirectional(LSTM(hidden_units, activation='tanh', input_shape=(seq_len,13), return_sequences=False, name='L{}'.format(i+2))))\n",
    "        else:\n",
    "            model.add(Bidirectional(LSTM(hidden_units, activation='tanh', return_sequences=True, name='L{}'.format(i+2))))\n",
    "    \n",
    "    model.add(Dense(latent_dim, name='latent'))\n",
    "    \n",
    "    z = Input(shape=(seq_len, 13), name='L1')\n",
    "    out = model(z)\n",
    "    return Model(z, out)\n",
    "\n",
    "\n",
    "def make_discriminator(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "#     model.add(RepeatVector(seq_len))\n",
    "#     model.add(Bidirectional(LSTM(10, activation='tanh', return_sequences=False, name='L2')))\n",
    "    model.add(Dense(13))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    z = Input(shape=(latent_dim,), name='L1')\n",
    "    out = model(z)\n",
    "    return Model(z, out)\n",
    "\n",
    "def make_supervisor(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "#     model.add(RepeatVector(seq_len))\n",
    "#     model.add(LSTM(50, activation='tanh',  return_sequences=False, name='L2'))\n",
    "\n",
    "    model.add(Dense(100, activation = 'tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(latent_dim))\n",
    "    z = Input(shape=(latent_dim,), name='L1')\n",
    "    v = Input(shape=(latent_dim,), name='L2')\n",
    "    \n",
    "    out = model(Concatenate()([z,v]))\n",
    "    return Model([z,v], out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c822e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "    epsilon = 1e-7  # small constant to avoid numerical instability\n",
    "    y_pred = K.clip(y_pred, epsilon, 1 - epsilon)  # clip y_pred to avoid log(0)\n",
    "    loss = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "    return K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32242994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true) - K.exp(y_true) + 1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66636caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4971eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_layers = 4\n",
    "d_model = 13\n",
    "dff = 100\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6c249e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_T = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=500,\n",
    "                               dropout_rate=dropout_rate, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2372d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (2, seq_len,  13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2b08455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 13)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f332933",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84816872",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=encoder_T(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea2d9ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 13])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f765e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lstm = make_encoder_lstm(seq_len, latent_dim, 3, 100)\n",
    "encoder_cnn = make_encoder_cnn(seq_len, latent_dim)\n",
    "decoder = make_decoder(seq_len, latent_dim)\n",
    "generator = make_generator(seq_len, latent_dim, 3, 500)\n",
    "supervisor = make_supervisor(seq_len, latent_dim)\n",
    "discriminator = make_discriminator(seq_len, latent_dim)\n",
    "\n",
    "z = Input(shape=(seq_len, 13))\n",
    "enc_tim = encoder_lstm(z)\n",
    "enc_spe = encoder_cnn(z)\n",
    "sup_e = supervisor([enc_tim, enc_spe])\n",
    "dec = decoder(sup_e)\n",
    "\n",
    "autoencoder = Model(z, dec)\n",
    "\n",
    "embedder = Model(z, sup_e)\n",
    "\n",
    "v = Input(shape=(seq_len, 13))\n",
    "gen_lstm = generator(v)\n",
    "gen_cnn = encoder_cnn(v)\n",
    "sup_z = supervisor([gen_lstm,gen_cnn])\n",
    "disc = discriminator(sup_z)\n",
    "gan = Model(v, disc)\n",
    "\n",
    "gen_sup = Model(v, sup_z)\n",
    "\n",
    "\n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer=Adam(0.002, 0.5))\n",
    "discriminator.compile(loss=sigmoid_binary_crossentropy, optimizer=Adam(0.002, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "# supervisor.trainable = False\n",
    "encoder_cnn.trainable = False\n",
    "# generator.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
    "gan.compile(loss=sigmoid_binary_crossentropy, optimizer=Adam(0.002, 0.5), metrics=['accuracy'])\n",
    "gen_sup.compile(loss=wasserstein_loss, optimizer=Adam(0.002, 0.5))\n",
    "\n",
    "f1 = decoder(sup_z)\n",
    "final = Model(v, f1)\n",
    "decoder.trainable = False\n",
    "supervisor.trainable = False\n",
    "final.compile(loss='mse', optimizer=Adam(0.002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "947f9162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36f1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_lstm(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc61843",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "num_epochs = 1001\n",
    "\n",
    "steps_per_epoch = int(len(LSTM_Train)/batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    v=0\n",
    "    L = np.random.permutation(len(LSTM_Train))\n",
    "    for batch in range(steps_per_epoch):\n",
    "        bs = batch_size\n",
    "        data = LSTM_Train[L[v:v+bs]]\n",
    "        a = autoencoder.train_on_batch(data, data)\n",
    "        final_data = autoencoder.predict(data)\n",
    "        real_data_x = embedder.predict(data)\n",
    "        v+=bs\n",
    "        bs =len(data)\n",
    "        noise = np.random.normal(0, 1, (bs, seq_len, 13))\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            real_loss = discriminator.train_on_batch(real_data_x, np.ones((bs,1)))\n",
    "            fake_data_z = gen_sup.predict(noise)\n",
    "            fake_loss = discriminator.train_on_batch(fake_data_z, np.zeros((bs,1)))\n",
    "            \n",
    "            discriminator_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "        \n",
    "        \n",
    "        l = gen_sup.train_on_batch(noise, real_data_x)\n",
    "\n",
    "\n",
    "        # Train the generator\n",
    "        generator_loss = gan.train_on_batch(noise, np.ones((bs, 1)))\n",
    "        final_loss = final.train_on_batch(noise,final_data)\n",
    "        if epoch==0 or (epoch+1)%10 == 0:\n",
    "            print(discriminator_loss, generator_loss)\n",
    "\n",
    "            # Print the loss\n",
    "            print('Epoch %d: Discriminator Loss=%.4f, Generator Loss=%.4f, Emb Loss=%.4f, Aut Loss=%.4f, Final Loss=%.4f' \n",
    "                  % (epoch+1, discriminator_loss[0], generator_loss[0], l, a, final_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa940953",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (len(LSTM_Train), seq_len,  13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = gen_sup.predict(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abed25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=final.predict(noise)\n",
    "Q = Q[:,-1]\n",
    "print(Q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = LSTM_Train[0]\n",
    "print(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = scaler.inverse_transform(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ae7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Temp   Hum   Pres  Rain  Light  Ax   Ay   Az  Wx   Wy   Wz   Mos   Mov')\n",
    "for i in range(13):\n",
    "    print(\"%.2f \"%(original_data[0][i]), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= embedder.predict(LSTM_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ce3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, LSTM_Train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df551370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE(A,LSTM_Train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.zeros(int(len(P)/2))\n",
    "M = np.ones(len(P)-int(len(P)/2))\n",
    "L = np.hstack((L,M))\n",
    "L = convert_to_one_hot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE(P,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e267670",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.vstack((A,P))\n",
    "zz = np.zeros((LSTM_Train_count.shape[0],1))\n",
    "zz = np.hstack((LSTM_Train_count,zz))\n",
    "cc = np.ones((L.shape[0],1))\n",
    "dd = np.zeros(L.shape)\n",
    "cc = np.hstack((dd,cc))\n",
    "ee = np.vstack((zz,cc))\n",
    "tSNE(B,ee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dcd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546124bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda9",
   "language": "python",
   "name": "tf-gpu-cuda9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
