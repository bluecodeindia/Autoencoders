{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec088de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ed2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['PYTHONHASHSEED']=str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense,Input,Reshape, Add, Flatten,ELU,RepeatVector,TimeDistributed, Bidirectional, PReLU, Concatenate, Subtract\n",
    "from keras.layers import LSTM, LayerNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, multiply, Multiply\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,UpSampling2D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Nadam\n",
    "from keras.models import clone_model\n",
    "from keras import backend as K\n",
    "import glob\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import r2_score\n",
    "from keras.utils import plot_model\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "from tensorflow.keras.layers import Add, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.stattools import pacf\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "# import seaborn as sns\n",
    "rcParams['figure.figsize']=15,5\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,7)})\n",
    "\n",
    "#For standardising the dat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd684fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(arr):\n",
    "    \"\"\"\n",
    "    Convert a given array into one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        arr (numpy.ndarray): Input array to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded array.\n",
    "    \"\"\"\n",
    "    # Get the unique elements in the input array\n",
    "    unique_elements = np.unique(arr)\n",
    "    \n",
    "    # Create a dictionary to map unique elements to one-hot vectors\n",
    "    one_hot_dict = {elem: np.eye(len(unique_elements))[i] for i, elem in enumerate(unique_elements)}\n",
    "    \n",
    "    # Map the input array to one-hot vectors\n",
    "    one_hot_arr = np.array([one_hot_dict[elem] for elem in arr])\n",
    "    \n",
    "    return one_hot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b398a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca384b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations = os.listdir('Clean-dataset-LMS')\n",
    "# Stations.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Column = ['Date','Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8818e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tSNE(Train, TrainL):\n",
    "    \"\"\"\n",
    "    Perform t-SNE dimensionality reduction on the given data and visualize it in an interactive 3D scatter plot using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        Train (numpy.ndarray): Input data array.\n",
    "        TrainL (numpy.ndarray): Labels for the input data array.\n",
    "    \"\"\"\n",
    "    x_subset = Train[:]\n",
    "    y_subset = [np.argmax(x) for x in TrainL[:]]\n",
    "\n",
    "    print(np.unique(y_subset))\n",
    "    if x_subset.ndim>=3:\n",
    "        x_subset = x_subset.reshape((x_subset.shape[0], x_subset.shape[1] * x_subset.shape[2]))\n",
    "    else:\n",
    "        x_subset=x_subset.reshape((x_subset.shape[0],x_subset.shape[1]))\n",
    "    \n",
    "    tsne = TSNE(random_state=42, n_components=3, verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset)\n",
    "    \n",
    "    # Create a 3D scatter plot using Scatter3d trace\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=tsne[:, 0],\n",
    "        y=tsne[:, 1],\n",
    "        z=tsne[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=y_subset,\n",
    "            colorscale='Spectral',\n",
    "            opacity=1,\n",
    "            line=dict(\n",
    "                width=0.5,\n",
    "                color='black'\n",
    "            )\n",
    "        ),\n",
    "        name='t-SNE'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "    title='Visualizing through t-SNE in 3D',\n",
    "    scene=dict(\n",
    "        xaxis_title='x',\n",
    "        yaxis_title='y',\n",
    "        zaxis_title='z'\n",
    "    ),\n",
    "    width=1000,  # Set width of the plot\n",
    "    height=800  # Set height of the plot\n",
    ")\n",
    "\n",
    "    # Create a separate plot with a colorbar to show the colors and their corresponding values\n",
    "    colorbar = go.Figure()\n",
    "    colorbar.add_trace(go.Scatter(\n",
    "        x=y_subset,\n",
    "        y=[0] * len(y_subset),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=y_subset,\n",
    "            colorscale='Spectral',\n",
    "            opacity=1,\n",
    "            colorbar=dict(\n",
    "                title='Class Label',\n",
    "                titleside='right',\n",
    "                tickvals=np.unique(y_subset),\n",
    "                ticktext=[str(i) for i in np.unique(y_subset)],\n",
    "                ticks='outside'\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    colorbar.update_layout(\n",
    "        title='Colorbar for t-SNE plot',\n",
    "        showlegend=False,\n",
    "        width=500,  # Set width of the plot\n",
    "        height=400  # Set height of the plot\n",
    "    )\n",
    "\n",
    "    # Display the plots side by side\n",
    "    \n",
    "    fig.show()\n",
    "#     colorbar.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe54daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(model, Train,TrainL,Test, TestL):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(15, 5), sharey='row')\n",
    "    Predict=[]\n",
    "    True_cls=[]\n",
    "    test=Train\n",
    "    y=TrainL\n",
    "\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict=np.argmax(P,axis=1)\n",
    "    True_cls=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls, Predict, labels=[0,1])\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp1.plot(ax=axes[0])\n",
    "    \n",
    "    disp1.im_.colorbar.remove()\n",
    "    train_acc = (cm[0][0]+cm[1][1])/np.sum(cm)\n",
    "\n",
    "    Predict2=[]\n",
    "    True_cls2=[]\n",
    "    test=Test\n",
    "    y=TestL\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict2=np.argmax(P,axis=1)\n",
    "    True_cls2=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls2, Predict2, labels=[0,1])\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp2.plot(ax=axes[1])\n",
    "    disp2.im_.colorbar.remove()\n",
    "    \n",
    "    # Evaluate model accuracy on training and testing sets\n",
    "    \n",
    "    test_acc = (cm[0][0]+cm[1][1])/np.sum(cm)\n",
    "\n",
    "    # Display accuracy values on subplots\n",
    "    axes[0].set_title('Training Set\\nAccuracy: {:.2f}%'.format(train_acc * 100))\n",
    "    axes[1].set_title('Testing Set\\nAccuracy: {:.2f}%'.format(test_acc * 100))\n",
    "\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the Array\n",
    "def makeArray(Array):\n",
    "    New=np.array(Array[0])\n",
    "\n",
    "    for i in range(1,len(Array)):\n",
    "        New = np.append(New,Array[i],axis=0)\n",
    "        \n",
    "    return New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(Stations):\n",
    "    \n",
    "    Data, C = [], []\n",
    "    \n",
    "#     print(Stations)\n",
    "    file = Stations\n",
    "    newfile = file\n",
    "    df = pd.read_csv('Clean-dataset-LMS/'+newfile, header=0, index_col=None)\n",
    "    print(newfile)\n",
    "    df = df.reset_index(drop=True)\n",
    "    data=df[['Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']].values\n",
    "    data=data.astype('float32')\n",
    "    data[np.where(data[:,-1]>0),-1] = 1\n",
    "    count=df['Count'].values\n",
    "    count[np.where(count>0)] = 1\n",
    "    count=count.astype('float32')\n",
    "    d = data[1:,5:8]-data[:-1,5:8]\n",
    "    d = np.where(np.abs(d) <= 0.02, 0, d)\n",
    "    w = data[1:,8:11]-data[:-1,8:11]\n",
    "    w[np.where(np.all(d == 0, axis=1))] = 0\n",
    "    count[np.where(np.any(d != 0, axis=1))] = 1\n",
    "    count[np.where(np.all(d == 0, axis=1))] = 0\n",
    "    \n",
    "    \n",
    "#     print(np.where(np.all(d == 0, axis=1)))\n",
    "#     print(np.std(w[:1000,0]),np.std(w[:1000,1]),np.std(w[:1000,2]))\n",
    "#     t = 2.0\n",
    "#     w[0] = np.where(np.abs(w[0]) <= t*np.std(w[:1000,0]), 0, w[0])\n",
    "#     w[1] = np.where(np.abs(w[1]) <= t*np.std(w[:1000,1]), 0, w[1])\n",
    "#     w[2] = np.where(np.abs(w[2]) <= t*np.std(w[:1000,2]), 0, w[2])\n",
    "    data = data[1:]\n",
    "    count = count[1:]\n",
    "    data[:,5:8]= d\n",
    "    data[:,8:11]= w\n",
    "\n",
    "    #Normalize the data\n",
    "    \n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    Data.append(data)\n",
    "    C.append(count)\n",
    "   \n",
    "    return makeArray(Data), makeArray(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c752dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=[[] for x in range(len(Stations))]\n",
    "Count=[[] for x in range(len(Stations))]\n",
    "for i in range(len(Stations)):\n",
    "    Data[i], Count[i] = readData(Stations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ea350",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(23):\n",
    "    idx = i\n",
    "    print(Data[idx].shape,np.unique(Data[idx],axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfc62d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(23):\n",
    "    plt.plot(Data[i][:,5:11])\n",
    "    plt.plot(Count[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db910d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packet(D, C, seq_len):\n",
    "    Packet_data = []\n",
    "    Packet_label = []\n",
    "    Packet_count = []\n",
    "\n",
    "    for i in range(len(D)-seq_len):\n",
    "        Packet_data.append(D[i:i+seq_len,:])\n",
    "        Packet_label.append(D[i+seq_len,:])\n",
    "        Packet_count.append(C[i+seq_len-1])\n",
    "\n",
    "    Packet_data = np.array(Packet_data)\n",
    "    Packet_label = np.array(Packet_label)\n",
    "    Packet_count = np.array(Packet_count).reshape(-1,1)\n",
    "    \n",
    "    return Packet_data, Packet_label, Packet_count\n",
    "\n",
    "def balance_data(d1, l1, c1):\n",
    "    P1 = []\n",
    "    L1 = []\n",
    "    C1 = []\n",
    "    count = 0\n",
    "    for i in range(len(c1)):\n",
    "        if c1[i]>0:\n",
    "            P1.append(d1[i])\n",
    "            L1.append(l1[i])\n",
    "            C1.append(c1[i])\n",
    "            count+=1\n",
    "\n",
    "            reset_random_seeds(seed)\n",
    "    idx = np.random.permutation(len(c1))        \n",
    "\n",
    "    P2 = []\n",
    "    L2 = []\n",
    "    C2 = []\n",
    "    idx1 = -1\n",
    "    while count!=0:\n",
    "        idx1+=1\n",
    "        if c1[idx[idx1]] == 0:\n",
    "            P2.append(d1[idx[idx1]])\n",
    "            L2.append(l1[idx[idx1]])\n",
    "            C2.append(c1[idx[idx1]])\n",
    "            count-=1\n",
    "\n",
    "    #Make the movement count to 1\n",
    "    #Comment this line if you want to movement count\n",
    "    C1=list(np.ones((len(C1))).reshape(-1,1))\n",
    "    \n",
    "    P1 = P1+P2\n",
    "    L1 = L1+L2\n",
    "    C1 = C1+C2\n",
    "    P1 = np.array(P1)\n",
    "    L1 = np.array(L1)\n",
    "    C1 = np.array(C1)\n",
    "    \n",
    "    return P1, L1, C1\n",
    "\n",
    "def mold(D):\n",
    "    T = []\n",
    "    for x in D:\n",
    "        for t in x:\n",
    "            T.append(t)\n",
    "            \n",
    "    return np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(Data, Count, seq_len):\n",
    "    print(\"Train:\")\n",
    "    print()\n",
    "    Train = []\n",
    "    Train_label = []\n",
    "    Train_count = []\n",
    "    S = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Train.append(P1)\n",
    "        Train_label.append(L1)\n",
    "        Train_count.append(C1)\n",
    "\n",
    "    print(\"Test:\")\n",
    "    print()\n",
    "    Test = []\n",
    "    Test_label = []\n",
    "    Test_count = []\n",
    "    S = [13,14,16,17,18,19,21,22]\n",
    "    for i in S:\n",
    "        D = Data[i]\n",
    "        C = Count[i]\n",
    "        d1, l1, c1 = make_packet(D, C, seq_len)\n",
    "        P1, L1, C1 = balance_data(d1, l1, c1)\n",
    "        print(C1.shape)\n",
    "        Test.append(P1)\n",
    "        Test_label.append(L1)\n",
    "        Test_count.append(C1)\n",
    "\n",
    "    Train = mold(Train)\n",
    "    Train_label = mold(Train_label)\n",
    "    Train_count = mold(Train_count)\n",
    "    Test = mold(Test)\n",
    "    Test_label = mold(Test_label)\n",
    "    Test_count = mold(Test_count)\n",
    "    \n",
    "    return Train, Train_label, Train_count, Test, Test_label, Test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Train, Auto_Train_label, Auto_Train_count, Auto_Test, Auto_Test_label, Auto_Test_count = train_test(Data, Count, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Train, LSTM_Train_label, LSTM_Train_count, LSTM_Test, LSTM_Test_label, LSTM_Test_count = train_test(Data, Count, seq_len)\n",
    "LSTM_Train_count1=[]\n",
    "\n",
    "for x in LSTM_Train_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Train_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Train_count1.append([0,1])\n",
    "LSTM_Train_count = np.array(LSTM_Train_count1) \n",
    "\n",
    "LSTM_Test_count1=[]\n",
    "\n",
    "for x in LSTM_Test_count:\n",
    "    if x[0]==0:\n",
    "        LSTM_Test_count1.append([1,0])\n",
    "    else:\n",
    "        LSTM_Test_count1.append([0,1])\n",
    "LSTM_Test_count = np.array(LSTM_Test_count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Reshape, Conv1DTranspose, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time-series data using the timeGAN model\n",
    "def generate_samples(timeGAN, latent_dim, num_samples):\n",
    "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
    "    generated = timeGAN.predict(noise)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66cae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (seq_len, 13)\n",
    "output_shape = input_shape\n",
    "latent_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ecb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedforward_network(input_dim, output_dim):\n",
    "    # Create the feedforward network\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add input layer and first hidden layer\n",
    "    model.add(Dense(units=64, activation='tanh', input_shape=(input_dim,)))\n",
    "\n",
    "    # Add additional hidden layers\n",
    "    model.add(Dense(units=128, activation='tanh'))\n",
    "    model.add(Dense(units=64, activation='tanh'))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93806f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x)\n",
    "#         x = self.add([x, attn_output])\n",
    "#         x = self.layernorm(x)\n",
    "        return attn_output\n",
    "\n",
    "def make_encoder_multihead(seq_len, dim, num_heads, key_dim):\n",
    "    input_shape = (seq_len, dim)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape, name='L10')\n",
    "    x = inputs\n",
    "\n",
    "    # Apply MultiHeadAttention\n",
    "    x = GlobalSelfAttention(num_heads=num_heads,key_dim=key_dim, dropout=0.1)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "#     x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "#     x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "    x = Dense(units=seq_len*dim, activation='sigmoid')(x)\n",
    "    x = Reshape((seq_len, dim))(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def make_encoder_multihead_d(seq_len, dim, num_heads, key_dim):\n",
    "    input_shape = (dim)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape, name='L11')\n",
    "    x = inputs\n",
    "    x = RepeatVector(seq_len)(x)\n",
    "\n",
    "    # Apply MultiHeadAttention\n",
    "    x = GlobalSelfAttention(num_heads=num_heads,key_dim=key_dim, dropout=0.1)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "    x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "#     x = Dense(units=seq_len* dim*2, activation='tanh')(x)\n",
    "    x = Dense(units=seq_len*dim, activation='sigmoid')(x)\n",
    "    x = Reshape((seq_len, dim))(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResidualConnection, self).__init__(**kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x, residual = inputs\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ebc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "    epsilon = 1e-7  # small constant to avoid numerical instability\n",
    "    y_pred = K.clip(y_pred, epsilon, 1 - epsilon)  # clip y_pred to avoid log(0)\n",
    "    loss = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "    return K.mean(loss, axis=-1)\n",
    "\n",
    "def kl_divergence_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true) - K.exp(y_true) + 1, axis=-1)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_lstm(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    z = Input(shape=(seq_len, 13), name='L9')\n",
    "    x = z\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers-1:\n",
    "            return_sequences = True\n",
    "        else:\n",
    "            return_sequences = True\n",
    "        \n",
    "        x_rnn =LSTM(hidden_units, activation='tanh',  return_sequences=return_sequences, name='R3{}'.format(i+2))(x)\n",
    "#         x_rnn = LayerNormalization()(x_rnn)\n",
    "        if return_sequences:\n",
    "            if i > 0 or x.shape[-1] == hidden_units:\n",
    "                x = Add()([x, x_rnn])\n",
    "            else:\n",
    "                x = x_rnn\n",
    "        else:\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "            x = Add()([Lambda(slice_last)(x), x_rnn])\n",
    "    \n",
    "    x_latent = TimeDistributed(Dense(13, activation=\"tanh\",name='latent'))(x)\n",
    "    out = x_latent\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_encoder_cnn(seq_len, latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, (3), activation=\"tanh\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Conv1D(32, (3), activation=\"tanh\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(seq_len*13,activation=\"tanh\"))\n",
    "    model.add(Reshape((seq_len,13)))\n",
    "    z = Input(shape=(seq_len,13), name='L8')\n",
    "    out = model(z)\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_decoder_cnn(seq_len, latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, (3), activation=\"tanh\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Conv1D(32, (3), activation=\"tanh\", padding=\"same\"))\n",
    "    model.add(MaxPooling1D((2), padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(seq_len*13,activation=\"tanh\"))\n",
    "    model.add(Reshape((seq_len,latent_dim)))\n",
    "    z = Input(shape=(seq_len,latent_dim), name='L8')\n",
    "    out = model(z)\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_decoder(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    z = Input(shape=(latent_dim), name='L7')\n",
    "    x = RepeatVector(seq_len)(z)\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            return_sequences = True\n",
    "        else:\n",
    "            return_sequences = True \n",
    "        \n",
    "        x_rnn = LSTM(hidden_units, activation='tanh', return_sequences=return_sequences, name='R1{}'.format(i+2))(x)\n",
    "#         x_rnn = LayerNormalization()(x_rnn)\n",
    "        if i > 0:\n",
    "            x = Add()([x, x_rnn])\n",
    "        else:\n",
    "            x = x_rnn\n",
    "    \n",
    "    x_latent = TimeDistributed(Dense(latent_dim, activation=\"tanh\", name='latent'))(x)\n",
    "    out = x_latent\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_generator(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    z = Input(shape=(seq_len, 13), name='L1')\n",
    "    x = z\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers-1:\n",
    "            return_sequences = True\n",
    "        else:\n",
    "            return_sequences = True\n",
    "        \n",
    "        x_rnn = LSTM(hidden_units, activation='tanh', return_sequences=return_sequences, name='R2{}'.format(i+2))(x)\n",
    "#         x_rnn = LayerNormalization()(x_rnn)\n",
    "        if return_sequences:\n",
    "            if i > 0 or x.shape[-1] == hidden_units:\n",
    "                x = Add()([x, x_rnn])\n",
    "            else:\n",
    "                x = x_rnn\n",
    "        else:\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "            x = Add()([Lambda(slice_last)(x), x_rnn])\n",
    "    \n",
    "    x_latent = TimeDistributed(Dense(13, activation=\"tanh\", name='latent'))(x)\n",
    "    out = x_latent\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_generator2(seq_len, latent_dim, num_layers, hidden_units):\n",
    "    z = Input(shape=(seq_len, 13), name='L1')\n",
    "    x = z\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers-1:\n",
    "            return_sequences = True\n",
    "        else:\n",
    "            return_sequences = True\n",
    "        \n",
    "        x_rnn = LSTM(hidden_units, activation='tanh', return_sequences=return_sequences, name='R2{}'.format(i+2))(x)\n",
    "#         x_rnn = LayerNormalization()(x_rnn)\n",
    "        if return_sequences:\n",
    "            if i > 0 or x.shape[-1] == hidden_units:\n",
    "                x = Add()([x, x_rnn])\n",
    "            else:\n",
    "                x = x_rnn\n",
    "        else:\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "            x = Add()([Lambda(slice_last)(x), x_rnn])\n",
    "    \n",
    "    x_latent = TimeDistributed(Dense(13, activation=\"tanh\",name='latent'))(x)\n",
    "    out = x_latent\n",
    "    \n",
    "    return Model(z, out)\n",
    "\n",
    "def make_discriminator(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(RepeatVector(seq_len))\n",
    "    model.add(Bidirectional(LSTM(50, activation='tanh', input_shape=(seq_len, latent_dim), return_sequences=False, name='L6')))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "#     model.add(Dense(13, activation='tanh'))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    z = Input(shape=(latent_dim), name='L5')\n",
    "    out = model(z)\n",
    "    return Model(z, out)\n",
    "\n",
    "def make_supervisor(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "#     model.add(RepeatVector(seq_len))\n",
    "    model.add(LSTM(500, activation='tanh',  return_sequences=False, name='L2'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(latent_dim))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(latent_dim))\n",
    "    z = Input(shape=(seq_len,13), name='L1')\n",
    "    v = Input(shape=(seq_len,13), name='L2')\n",
    "    u = Input(shape=(seq_len,13), name='L23')\n",
    "    \n",
    "    x = Add()([z,v])\n",
    "    x = Add()([x,u])\n",
    "    out = model(x)\n",
    "    return Model([z, v, u], out)\n",
    "\n",
    "def make_decoder2(seq_len, latent_dim):\n",
    "    model = Sequential()\n",
    "#     model.add(RepeatVector(seq_len))\n",
    "    model.add(LSTM(100, activation='tanh',  input_shape = (seq_len,latent_dim), return_sequences=True, name='L2'))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(50, activation='tanh')))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(seq_len*13, activation='sigmoid'))\n",
    "    model.add(Reshape((seq_len,13)))\n",
    "    z = Input(shape=(seq_len,latent_dim), name='L3')\n",
    "    v = Input(shape=(seq_len,latent_dim), name='L4')\n",
    "    \n",
    "    out = model(Add()([z,v]))\n",
    "    return Model([z,v], out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_random_seeds(seed)\n",
    "lstm_layer = 2\n",
    "units = 50\n",
    "encoder_lstm = make_encoder_lstm(seq_len, latent_dim, lstm_layer, units)\n",
    "encoder_cnn = make_encoder_cnn(seq_len, latent_dim)\n",
    "# decoder_cnn = make_decoder_cnn(seq_len, latent_dim)\n",
    "decoder = make_decoder(seq_len, latent_dim, lstm_layer, units)\n",
    "decoder2 = make_decoder2(seq_len, latent_dim)\n",
    "generator = make_generator(seq_len, latent_dim, lstm_layer, units)\n",
    "generator2 = make_generator(seq_len, latent_dim, lstm_layer, units)\n",
    "supervisor = make_supervisor(seq_len, latent_dim)\n",
    "discriminator = make_discriminator(seq_len, latent_dim)\n",
    "attention_e = make_encoder_multihead(seq_len, 13, 4, 32)\n",
    "attention_d = make_encoder_multihead_d(seq_len, latent_dim, 4, 32)\n",
    "\n",
    "z = Input(shape=(seq_len, 13))\n",
    "enc_tim = encoder_lstm(z)\n",
    "enc_spe = encoder_cnn(z)\n",
    "enc_att = attention_e(z)\n",
    "sup_e = supervisor([enc_tim, enc_spe, enc_att])\n",
    "dec1 = decoder(sup_e)\n",
    "# dec2 = decoder_cnn(sup_e)\n",
    "dec3 = attention_d(sup_e)\n",
    "\n",
    "dec = decoder2([dec1, dec3])\n",
    "\n",
    "autoencoder = Model(z, dec)\n",
    "\n",
    "embedder = Model(z, sup_e)\n",
    "\n",
    "v = Input(shape=(seq_len, 13))\n",
    "gen_lstm = generator(v)\n",
    "gen_spe = encoder_cnn(v)\n",
    "gen_cnn = attention_e(v)\n",
    "sup_z = supervisor([gen_lstm, gen_spe, gen_cnn])\n",
    "disc = discriminator(sup_z)\n",
    "gan = Model(v, disc)\n",
    "\n",
    "gen_sup = Model(v, sup_z)\n",
    "\n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer=Adam(0.02, 0.5))\n",
    "discriminator.compile(loss=sigmoid_binary_crossentropy, optimizer=Adam(0.02, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "# supervisor.trainable = False\n",
    "encoder_cnn.trainable = False\n",
    "attention_e.trainable = False\n",
    "attention_d.trainable = False\n",
    "# generator.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "\n",
    "\n",
    "f1 = decoder(sup_z)\n",
    "f2 = attention_d(sup_z)\n",
    "f3 = decoder2([f1, f2])\n",
    "final = Model(v, f3)\n",
    "decoder.trainable = False\n",
    "decoder2.trainable = False\n",
    "# supervisor.trainable = False\n",
    "final.compile(loss='mse', optimizer=Adam(0.02, 0.5))\n",
    "\n",
    "\n",
    "\n",
    "# supervisor.trainable = False\n",
    "gen_sup.compile(loss='mse', optimizer=Adam(0.02, 0.5))\n",
    "# generator.trainable = False\n",
    "gan.compile(loss=sigmoid_binary_crossentropy, optimizer=Adam(0.02, 0.5), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(encoder_lstm, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LSTM_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabb299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "num_epochs = 5001\n",
    "final_loss = 0\n",
    "emb_loss = 0\n",
    "auto_loss = 0\n",
    "gen_his = 10\n",
    "lock = True\n",
    "min_loss = 0.0\n",
    "max_loss = 1.0\n",
    "discriminator_loss = [0,0]\n",
    "generator_loss = [0,0]\n",
    "steps_per_epoch = int(len(LSTM_Train)/batch_size)\n",
    "\n",
    "# reset_random_seeds(seed)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    v=0\n",
    "    L = np.random.permutation(len(LSTM_Train))\n",
    "    for batch in range(steps_per_epoch):\n",
    "        \n",
    "        bs = batch_size\n",
    "        if v+bs>len(LSTM_Train):\n",
    "            bs = len(LSTM_Train)-v\n",
    "        data = LSTM_Train[v:v+bs]\n",
    "        v+=bs\n",
    "        bs =len(data)\n",
    "        \n",
    "        \n",
    "        auto_loss = autoencoder.train_on_batch(data, data)\n",
    "        \n",
    "        real_data_x = embedder.predict(data)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (bs,seq_len,13))\n",
    "#         noise = scaler.fit_transform(noise)\n",
    "#         noise = noise.reshape((bs, seq_len,  13))\n",
    "        \n",
    "    \n",
    "        \n",
    "        gen_sup.train_on_batch(noise,real_data_x)\n",
    "        \n",
    "        \n",
    "        generator_loss = gan.train_on_batch(noise, np.ones((bs, 1)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            \n",
    "            \n",
    "            real_loss = discriminator.train_on_batch(real_data_x, np.ones((bs,1)))\n",
    "            fake_data_z = gen_sup.predict(noise)\n",
    "            fake_loss = discriminator.train_on_batch(fake_data_z, np.zeros((bs,1)))\n",
    "            discriminator_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "            \n",
    "                \n",
    "                \n",
    "                      \n",
    "        if epoch==0 or (epoch+1)%10 == 0:\n",
    "            print(real_loss[1], fake_loss[1], discriminator_loss, generator_loss)\n",
    "\n",
    "            # Print the loss\n",
    "            print('Epoch %d: Discriminator Loss=%.4f, Generator Loss=%.4f, Emb Loss=%.4f, Aut Loss=%.4f, Final Loss=%.4f' \n",
    "                  % (epoch+1, discriminator_loss[0], generator_loss[0], emb_loss, auto_loss, final_loss))\n",
    "            \n",
    "        if epoch%100==0:\n",
    "            P = gen_sup.predict(noise)\n",
    "            A= embedder.predict(LSTM_Train)\n",
    "            L = np.zeros(int(len(P)/2))\n",
    "            M = np.ones(len(P)-int(len(P)/2))\n",
    "            L = np.hstack((L,M))\n",
    "            L = convert_to_one_hot(L)\n",
    "            B = np.vstack((A,P))\n",
    "            zz = np.zeros((LSTM_Train_count.shape[0],1))\n",
    "            zz = np.hstack((LSTM_Train_count,zz))\n",
    "            cc = np.ones((L.shape[0],1))\n",
    "            dd = np.zeros(L.shape)\n",
    "            cc = np.hstack((dd,cc))\n",
    "            ee = np.vstack((zz,cc))\n",
    "            tSNE(B,ee)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961529e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (len(LSTM_Train),seq_len,13))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = gen_sup.predict(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=final.predict(noise)\n",
    "Q = Q[:,-1]\n",
    "print(Q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcec363",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = LSTM_Train[0]\n",
    "print(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = scaler.inverse_transform(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Temp   Hum   Pres  Rain  Light  Ax   Ay   Az  Wx   Wy   Wz   Mos   Mov')\n",
    "for i in range(13):\n",
    "    print(\"%.2f \"%(original_data[0][i]), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a470374",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= embedder.predict(LSTM_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc97728",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, LSTM_Train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE(A,LSTM_Train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.zeros(int(len(P)/2))\n",
    "M = np.ones(len(P)-int(len(P)/2))\n",
    "L = np.hstack((L,M))\n",
    "L = convert_to_one_hot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a84779",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE(P,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.vstack((A,P))\n",
    "zz = np.zeros((LSTM_Train_count.shape[0],1))\n",
    "zz = np.hstack((LSTM_Train_count,zz))\n",
    "cc = np.ones((L.shape[0],1))\n",
    "dd = np.zeros(L.shape)\n",
    "cc = np.hstack((dd,cc))\n",
    "ee = np.vstack((zz,cc))\n",
    "tSNE(B,ee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09feaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18795b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e21b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22e898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01a912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df565f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a70a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf5e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda9",
   "language": "python",
   "name": "tf-gpu-cuda9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
