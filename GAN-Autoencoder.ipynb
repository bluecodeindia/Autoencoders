{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c9ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295fab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['PYTHONHASHSEED']=str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9bcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8aead8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense,Input,Reshape, Flatten,ELU,RepeatVector,TimeDistributed, Bidirectional, PReLU, Concatenate, Subtract\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, multiply\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "import glob\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import r2_score\n",
    "from keras.utils import plot_model\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from keras.utils import to_categorical\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.stattools import pacf\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "# import seaborn as sns\n",
    "rcParams['figure.figsize']=15,5\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcfd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,7)})\n",
    "\n",
    "#For standardising the dat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7683c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb39076",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations = ['Batseri kinnaur','Gharpa','ghoda_farm3_mandi','griffon peak 2','griffon_peak3',\n",
    "            'kuppa_data','nigulasridata','pagalnala_data','purbani_kinnaur','sanarli_1_mandi','sanarli_3_mandi',\n",
    "            'sandhol kangra','urni_dhank_kinnaur','griffon peak5 mandi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b0cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Column = ['Date','Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe14652",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5768997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the Array\n",
    "def makeArray(Array):\n",
    "    New=np.array(Array[0])\n",
    "\n",
    "    for i in range(1,len(Array)):\n",
    "        New = np.append(New,Array[i],axis=0)\n",
    "        \n",
    "    return New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a8a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(Stations):\n",
    "    \n",
    "    Data, C = [], []\n",
    "    \n",
    "#     print(Stations)\n",
    "    file = Stations+'.csv'\n",
    "    newfile = 'clean_'+file\n",
    "    df = pd.read_csv('Clean-dataset-LMS/'+newfile, header=0, index_col=None)\n",
    "    print(newfile)\n",
    "    df = df.reset_index(drop=True)\n",
    "    data=df[['Tem','Hum','Pressure','Rain','Light','Ax','Ay','Az','Wx','Wy','Wz','Moisture','Count']].values\n",
    "    data=data.astype('float32')\n",
    "    count=df['Count'].values\n",
    "    count=count.astype('float32')\n",
    "\n",
    "    #Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    Data.append(data)\n",
    "    C.append(count)\n",
    "           \n",
    "    \n",
    "        \n",
    "    return makeArray(Data), makeArray(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cecbcf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_Batseri kinnaur.csv\n",
      "clean_Gharpa.csv\n",
      "clean_ghoda_farm3_mandi.csv\n",
      "clean_griffon peak 2.csv\n",
      "clean_griffon_peak3.csv\n",
      "clean_kuppa_data.csv\n",
      "clean_nigulasridata.csv\n",
      "clean_pagalnala_data.csv\n",
      "clean_purbani_kinnaur.csv\n",
      "clean_sanarli_1_mandi.csv\n",
      "clean_sanarli_3_mandi.csv\n",
      "clean_sandhol kangra.csv\n",
      "clean_urni_dhank_kinnaur.csv\n",
      "clean_griffon peak5 mandi.csv\n"
     ]
    }
   ],
   "source": [
    "Data=[[] for x in range(len(Stations))]\n",
    "Count=[[] for x in range(len(Stations))]\n",
    "for i in range(len(Stations)):\n",
    "    Data[i], Count[i] = readData(Stations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef44d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(data, C, lag,z):\n",
    "    DataX, DataY = [], []\n",
    "    count=0\n",
    "    for i in range(lag,len(data)):\n",
    "        if C[i]>0:\n",
    "            count+=1\n",
    "            DataX.append(data[i-lag:i,:])\n",
    "            \n",
    "    \n",
    "    countnew=0\n",
    "    idx=lag\n",
    "    np.random.seed(z)\n",
    "    per = np.random.permutation(len(data))\n",
    "    for i in per:\n",
    "\n",
    "        if C[i]==0:\n",
    "            if sum(C[lag-i:i])==0:\n",
    "                countnew+=1\n",
    "                DataY.append(data[i-lag:i,:])\n",
    "                \n",
    "        if countnew==count:\n",
    "            break\n",
    "    DataX=np.array(DataX).astype(np.float32)   \n",
    "    DataY=np.array(DataY).astype(np.float32)\n",
    "    print(DataX.shape,DataY.shape)\n",
    "    return DataX, DataY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288196b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeData(Data,Count,lag):\n",
    "    \n",
    "    #Make the packets by lag value\n",
    "    f = True\n",
    "    g=0\n",
    "    h=0\n",
    "    y=0\n",
    "    l=len(Count)\n",
    "    R=[1998,7161,8200,1461,7168,5312,2202,8668,9335,6188,6934,6314,4156,5377]\n",
    "    while(f and g<l):\n",
    "        try:\n",
    "            DataX, DataY = getData(Data, Count, lag, R[h])\n",
    "            f=False\n",
    "        except:\n",
    "            g+=1\n",
    "            h+=1\n",
    "            h%=14\n",
    "            x=int(g/l*100)\n",
    "            if x-y!=0:\n",
    "                print(str(x)+', ',end='')\n",
    "            y=x\n",
    "\n",
    "#     print(len(DataX),len(DataY))\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    per = np.random.permutation(len(DataX))\n",
    "    Mov = DataX[:]\n",
    "    NonMov = DataY[:]\n",
    "    \n",
    "    \n",
    "    #Creating labels\n",
    "    A=np.ones(len(Mov))\n",
    "    B=np.zeros(len(NonMov))\n",
    "    \n",
    "       \n",
    "\n",
    "    #Creating One-Hot-Encoding\n",
    "    C=np.hstack((A,B))\n",
    "    D=to_categorical(C, dtype =\"uint8\")\n",
    "    E=D[:len(Mov)]\n",
    "    F=D[len(Mov):]\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    return (Mov, E), (NonMov, F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bcdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(Train):\n",
    "    T=[]\n",
    "    for x in Train:\n",
    "        for y in x:\n",
    "            T.append(y)\n",
    "    return np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d889e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anotherLevel(encoded_Data,Count,lag):\n",
    "    Mov=[]\n",
    "    Mov_L=[]\n",
    "    NoMov= []\n",
    "    NoMov_L= []\n",
    "\n",
    "    for i in range(len(Stations)):\n",
    "        if sum(Count[i][lag:])>0:\n",
    "            (A, B),(C, D) = makeData(encoded_Data[i], Count[i],lag)\n",
    "            Mov.append(A)\n",
    "            NoMov.append(C)\n",
    "            Mov_L.append(B)\n",
    "            NoMov_L.append(D)\n",
    "\n",
    "    total = len(Mov)\n",
    "\n",
    "    Train =[]\n",
    "    Test =[]\n",
    "    TrainL =[]\n",
    "    TestL =[]\n",
    "    Movement=[]\n",
    "    NoMovement=[]\n",
    "    MovementL=[]\n",
    "    NoMovementL=[]\n",
    "\n",
    "    for i in range(total):\n",
    "\n",
    "        MM = Mov[i][:]\n",
    "        NN = NoMov[i][:]\n",
    "        OO = Mov_L[i][:]\n",
    "        PP = NoMov_L[i][:]\n",
    "        Movement.append(MM)\n",
    "        NoMovement.append(NN)\n",
    "        MovementL.append(OO)\n",
    "        NoMovementL.append(PP)\n",
    "\n",
    "    \n",
    "    Movement=comp(Movement) \n",
    "    NoMovement=comp(NoMovement)\n",
    "    MovementL=comp(MovementL)\n",
    "    NoMovementL=comp(NoMovementL)\n",
    "    \n",
    "    t=int(len(Movement)*0.2)\n",
    "    np.random.seed(seed)\n",
    "    per=np.random.permutation(len(Movement))\n",
    "    per=per[:t]\n",
    "    for i in range(len(Movement)):\n",
    "        if i in per:\n",
    "            Test.append(Movement[i])\n",
    "            Test.append(NoMovement[i])\n",
    "            TestL.append(MovementL[i])\n",
    "            TestL.append(NoMovementL[i])\n",
    "        else:\n",
    "            Train.append(Movement[i])\n",
    "            Train.append(NoMovement[i])\n",
    "            TrainL.append(MovementL[i])\n",
    "            TrainL.append(NoMovementL[i])\n",
    "    \n",
    "    Train=np.array(Train)\n",
    "    TrainL=np.array(TrainL)\n",
    "    Test=np.array(Test)\n",
    "    TestL=np.array(TestL)\n",
    "    return (Train,TrainL),(Test,TestL)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17dd08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, p):\n",
    "        super(NoiseLayer, self).__init__()\n",
    "        self.p = p\n",
    "        \n",
    "        \n",
    "    def call(self, inputs,training=False):\n",
    "\n",
    "        if training:\n",
    "                    \n",
    "\n",
    "            Gaussioan_noise = tf.random.normal( shape=tf.shape(inputs), mean=0, stddev=0.01, dtype=inputs.dtype)\n",
    "            Gaussioan_noise=tf.nn.dropout(Gaussioan_noise,1-self.p)\n",
    "\n",
    "            inputs = inputs+Gaussioan_noise\n",
    "\n",
    "            inputs=inputs/tf.reduce_max(inputs,axis=1, keepdims=True)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641bef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tSNE(Train,TrainL,Test,TestL):\n",
    "    \n",
    "    x_subset = Train[:]\n",
    "    y_subset = [np.argmax(x) for x in TrainL[:]]\n",
    "\n",
    "    x_subset1 = Test[:]\n",
    "    y_subset1 = [np.argmax(x) for x in TestL[:]]\n",
    "\n",
    "    print(np.unique(y_subset))\n",
    "    x_subset=x_subset.reshape((x_subset.shape[0],x_subset.shape[1]*x_subset.shape[2]))\n",
    "    x_subset1=x_subset1.reshape((x_subset1.shape[0],x_subset1.shape[1]*x_subset1.shape[2]))\n",
    "    %time\n",
    "    tsne = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset)\n",
    "    tsne2 = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset1)\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], s= 50, c=y_subset, cmap='Spectral',edgecolors='black')\n",
    "    plt.scatter(tsne2[:, 0], tsne2[:, 1], s= 50, c=y_subset1, cmap='spring',edgecolors='black')\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.colorbar(boundaries=np.arange(2)-0.5).set_ticks(np.arange(2))\n",
    "    plt.title('Visualizing through t-SNE', fontsize=24);\n",
    "\n",
    "def tSNE2(Train,TrainL,Test,TestL):\n",
    "    \n",
    "    x_subset = Train[:]\n",
    "    y_subset = [np.argmax(x) for x in TrainL[:]]\n",
    "\n",
    "    x_subset1 = Test[:]\n",
    "    y_subset1 = [np.argmax(x) for x in TestL[:]]\n",
    "\n",
    "    print(np.unique(y_subset))\n",
    "    x_subset=x_subset.reshape((x_subset.shape[0],x_subset.shape[1]))\n",
    "    x_subset1=x_subset1.reshape((x_subset1.shape[0],x_subset1.shape[1]))\n",
    "    %time\n",
    "    tsne = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset)\n",
    "    tsne2 = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset1)\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], s= 50, c=y_subset, cmap='Spectral',edgecolors='black')\n",
    "    plt.scatter(tsne2[:, 0], tsne2[:, 1], s= 50, c=y_subset1, cmap='spring',edgecolors='black')\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.colorbar(boundaries=np.arange(2)-0.5).set_ticks(np.arange(2))\n",
    "    plt.title('Visualizing through t-SNE', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62157a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(Train,TrainL,Test, TestL,TrainOne,TrainOneL,TestOne, TestOneL):\n",
    "    f, axes = plt.subplots(1, 4, figsize=(15, 5), sharey='row')\n",
    "    Predict=[]\n",
    "    True_cls=[]\n",
    "    test=Train\n",
    "    y=TrainL\n",
    "\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict=np.argmax(P,axis=1)\n",
    "    True_cls=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls, Predict, labels=[0,1])\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp1.plot(ax=axes[0])\n",
    "    disp1.im_.colorbar.remove()\n",
    "\n",
    "    Predict2=[]\n",
    "    True_cls2=[]\n",
    "    test=Test\n",
    "    y=TestL\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict2=np.argmax(P,axis=1)\n",
    "    True_cls2=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls2, Predict2, labels=[0,1])\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp2.plot(ax=axes[1])\n",
    "    disp2.im_.colorbar.remove()\n",
    "\n",
    "    test=TrainOne\n",
    "    y=TrainOneL\n",
    "\n",
    "    P=model2.predict(test,verbose=0)\n",
    "    Predict3=np.argmax(P,axis=1)\n",
    "    True_cls3=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls3, Predict3, labels=[0,1])\n",
    "    disp3 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp3.plot(ax=axes[2])\n",
    "    disp3.im_.colorbar.remove()\n",
    "\n",
    "\n",
    "    test=TestOne\n",
    "    y=TestOneL\n",
    "    P=model2.predict(test,verbose=0)\n",
    "    Predict4=np.argmax(P,axis=1)\n",
    "    True_cls4=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls4, Predict4, labels=[0,1])\n",
    "    disp4 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp4.plot(ax=axes[3])\n",
    "    disp4.im_.colorbar.remove()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def matrix1(model, Train,TrainL,Test, TestL):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(15, 5), sharey='row')\n",
    "    Predict=[]\n",
    "    True_cls=[]\n",
    "    test=Train\n",
    "    y=TrainL\n",
    "\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict=np.argmax(P,axis=1)\n",
    "    True_cls=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls, Predict, labels=[0,1])\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp1.plot(ax=axes[0])\n",
    "    disp1.im_.colorbar.remove()\n",
    "\n",
    "    Predict2=[]\n",
    "    True_cls2=[]\n",
    "    test=Test\n",
    "    y=TestL\n",
    "    P=model.predict(test,verbose=0)\n",
    "    Predict2=np.argmax(P,axis=1)\n",
    "    True_cls2=np.argmax(y,axis=1)\n",
    "\n",
    "    cm = confusion_matrix(True_cls2, Predict2, labels=[0,1])\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['No Mov','Mov'])\n",
    "    disp2.plot(ax=axes[1])\n",
    "    disp2.im_.colorbar.remove()\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf2fbe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 13) (3, 10, 13)\n",
      "(1, 10, 13) (1, 10, 13)\n",
      "(475, 10, 13) (475, 10, 13)\n",
      "(464, 10, 13) (464, 10, 13)\n",
      "(10, 10, 13) (10, 10, 13)\n",
      "(2, 10, 13) (2, 10, 13)\n",
      "(2, 10, 13) (2, 10, 13)\n",
      "(1, 10, 13) (1, 10, 13)\n",
      "(3, 10, 13) (3, 10, 13)\n"
     ]
    }
   ],
   "source": [
    "(Train,TrainL),(Test,TestL) = anotherLevel(Data,Count,lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1bb7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15380, 13), (3840, 13))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft=Data[0].shape[1]\n",
    "\n",
    "TrainX = Train.reshape((-1,ft))\n",
    "TestX = Test.reshape((-1,ft))\n",
    "TrainX.shape,TestX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b689153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxvalue=max(np.max(TrainX),np.max(TestX))\n",
    "minvalue=min(np.min(TrainX),np.min(TestX))\n",
    "TrainX = (TrainX-minvalue)/(maxvalue-minvalue)\n",
    "TestX = (TestX-minvalue)/(maxvalue-minvalue)\n",
    "maxvalue,minvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acbc9c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 13) (3, 10, 13)\n",
      "(1, 10, 13) (1, 10, 13)\n",
      "(475, 10, 13) (475, 10, 13)\n",
      "(464, 10, 13) (464, 10, 13)\n",
      "(10, 10, 13) (10, 10, 13)\n",
      "(2, 10, 13) (2, 10, 13)\n",
      "(2, 10, 13) (2, 10, 13)\n",
      "(1, 10, 13) (1, 10, 13)\n",
      "(3, 10, 13) (3, 10, 13)\n"
     ]
    }
   ],
   "source": [
    "(TrainOne,TrainOneL),(TestOne,TestOneL) = anotherLevel(Data,Count,lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4c8eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538, 10, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainOne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a758fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = TrainOne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59d3f283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538, 10, 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6962fe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538, 10, 13)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainOne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef9d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = TrainOne\n",
    "test_x = TestOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "616369a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random_seeds(seed)\n",
    "\n",
    "def sample_normal(latent_dim, batch_size, window_size=None):\n",
    "    shape = (batch_size, latent_dim) if window_size is None else (batch_size, window_size, latent_dim)\n",
    "    return np.random.normal(size=shape)\n",
    "  \n",
    "def sample_categories(cat_dim, batch_size):\n",
    "    cats = np.zeros((batch_size, cat_dim))\n",
    "    for i in range(batch_size):\n",
    "        one = np.random.randint(0, cat_dim)\n",
    "        cats[i][one] = 1\n",
    "    return cats\n",
    "\n",
    "def create_encoder(latent_dim, cat_dim, window_size, input_dim):\n",
    "    input_layer = Input(shape=(window_size, input_dim))\n",
    "    \n",
    "    code = TimeDistributed(Dense(12, activation='linear'))(input_layer)\n",
    "    code = Bidirectional(LSTM(5, return_sequences=True))(code)\n",
    "    code = BatchNormalization()(code)\n",
    "    code = ELU()(code)\n",
    "    code = Bidirectional(LSTM(10))(code)\n",
    "    code = BatchNormalization()(code)\n",
    "    code = ELU()(code)\n",
    "    \n",
    "    cat = Dense(64)(code)\n",
    "    cat = BatchNormalization()(cat)\n",
    "    cat = PReLU()(cat)\n",
    "    cat = Dense(cat_dim, activation='softmax')(cat)\n",
    "    \n",
    "    latent_repr = Dense(10)(code)\n",
    "    latent_repr = BatchNormalization()(latent_repr)\n",
    "    latent_repr = PReLU()(latent_repr)\n",
    "    latent_repr = Dense(latent_dim)(latent_repr)\n",
    "    \n",
    "    decode = Concatenate()([latent_repr, cat])\n",
    "    decode = RepeatVector(window_size)(decode)\n",
    "    decode = Bidirectional(LSTM(10, return_sequences=True))(decode)\n",
    "    decode = ELU()(decode)\n",
    "    decode = Bidirectional(LSTM(5, return_sequences=True))(decode)\n",
    "    decode = ELU()(decode)\n",
    "    decode = TimeDistributed(Dense(12))(decode)\n",
    "    decode = ELU()(decode)\n",
    "    decode = TimeDistributed(Dense(input_dim, activation='linear'))(decode)\n",
    "    \n",
    "    error = Subtract()([input_layer, decode])\n",
    "        \n",
    "    return Model(input_layer, [decode, latent_repr, cat, error])\n",
    "\n",
    "def create_discriminator(latent_dim):\n",
    "    input_layer = Input(shape=(latent_dim,))\n",
    "    disc = Dense(128)(input_layer)\n",
    "    disc = ELU()(disc)\n",
    "    disc = Dense(64)(disc)\n",
    "    disc = ELU()(disc)\n",
    "    disc = Dense(1, activation=\"sigmoid\")(disc)\n",
    "    \n",
    "    model = Model(input_layer, disc)\n",
    "    return model\n",
    "\n",
    "window_size = train_x.shape[1]\n",
    "input_dim = train_x.shape[2]\n",
    "latent_dim = 5\n",
    "cat_dim = 2\n",
    "\n",
    "prior_discriminator = create_discriminator(latent_dim)\n",
    "prior_discriminator.compile(loss='binary_crossentropy', \n",
    "                            optimizer='adam', \n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "prior_discriminator.trainable = False\n",
    "\n",
    "cat_discriminator = create_discriminator(cat_dim)\n",
    "cat_discriminator.compile(loss='binary_crossentropy', \n",
    "                          optimizer='adam', \n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "cat_discriminator.trainable = False\n",
    "\n",
    "encoder = create_encoder(latent_dim, cat_dim, window_size, input_dim)\n",
    "\n",
    "signal_in = Input(shape=(window_size, input_dim))\n",
    "reconstructed_signal, encoded_repr, category, _ = encoder(signal_in)\n",
    "\n",
    "is_real_prior = prior_discriminator(encoded_repr)\n",
    "is_real_cat = cat_discriminator(category)\n",
    "\n",
    "autoencoder = Model(signal_in, [reconstructed_signal, is_real_prior, is_real_cat])\n",
    "autoencoder.compile(loss=['mse', 'binary_crossentropy', 'binary_crossentropy'],\n",
    "                                loss_weights=[0.99, 0.005, 0.005],\n",
    "                                optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509f15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(encoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1a87243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1000, Acc. Prior/Cat: 91.41% / 71.48%] [MSE train/val: 0.004593 / 0.000000]: 10\n"
     ]
    }
   ],
   "source": [
    "reset_random_seeds(seed)\n",
    "\n",
    "\n",
    "batches = 1000\n",
    "batch_size=128\n",
    "\n",
    "losses_disc = []\n",
    "losses_disc_cat = []\n",
    "losses_ae = []\n",
    "losses_val = []\n",
    "\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "def discriminator_training(discriminator, real, fake):\n",
    "    def train(real_samples, fake_samples):\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        loss_real = discriminator.train_on_batch(real_samples, real)\n",
    "        loss_fake = discriminator.train_on_batch(fake_samples, fake)\n",
    "        loss = np.add(loss_real, loss_fake) * 0.5\n",
    "\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        return loss\n",
    "    return train\n",
    "\n",
    "train_prior_discriminator = discriminator_training(prior_discriminator, real, fake)\n",
    "train_cat_discriminator = discriminator_training(cat_discriminator, real, fake)\n",
    "\n",
    "pbar = tqdm(range(batches))\n",
    "cnt=0\n",
    "for _ in pbar:\n",
    "    cnt+=1\n",
    "    ids = np.random.randint(0, train_x.shape[0], batch_size)\n",
    "    signals = train_x[ids]\n",
    "    idx = np.random.randint(0, test_x.shape[0], batch_size)\n",
    "    signalx = test_x[idx]\n",
    "\n",
    "    _, latent_fake, category_fake, _ = encoder.predict(signals)\n",
    "\n",
    "    latent_real = sample_normal(latent_dim, batch_size)\n",
    "    category_real = sample_categories(cat_dim, batch_size)\n",
    "\n",
    "    prior_loss = train_prior_discriminator(latent_real, latent_fake)\n",
    "    cat_loss = train_cat_discriminator(category_real, category_fake)\n",
    "\n",
    "    losses_disc.append(prior_loss)\n",
    "    losses_disc_cat.append(cat_loss)\n",
    "\n",
    "    encoder_loss = autoencoder.train_on_batch(signals, [signals, real, real])\n",
    "    losses_ae.append(encoder_loss)\n",
    "\n",
    "#     val_loss = autoencoder.test_on_batch(signalx, [signalx, real, real])\n",
    "    val_loss=0\n",
    "    losses_val.append(val_loss)\n",
    "    \n",
    "    \n",
    "    pbar.set_description(\"[%d, Acc. Prior/Cat: %.2f%% / %.2f%%] [MSE train/val: %f / %f]\" \n",
    "            % (cnt,100*prior_loss[1], 100*cat_loss[1], encoder_loss[1], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93caeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.save_weights('aae.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45568dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fb61ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dec, rep1, cat, error) = encoder.predict(train_x)\n",
    "(dec, rep2, cat, error) = encoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47760119",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random_seeds(seed)\n",
    "\n",
    "input_gen1 = Input(shape=(5,))\n",
    "xx = Dense(64,  activation='relu')(input_gen1)\n",
    "xx = Dense(512, activation='relu')(xx)\n",
    "# xx = Dense(512, activation='relu')(xx)\n",
    "xx = Dense(2, activation='softmax')(xx)\n",
    "classifier = Model(input_gen1, xx)\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "callback=keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=1.0e-4, \n",
    "                                          patience=10, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
    "classifier.fit(rep1,TrainOneL, epochs=500, batch_size=2048, validation_data=(rep2, TestOneL),callbacks=callback, verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e385791",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(rep1,TrainOneL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc613e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(rep2,TestOneL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1(classifier,rep1, TrainOneL,rep2,TestOneL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb6899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39800a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda9",
   "language": "python",
   "name": "tf-gpu-cuda9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
